{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moeez774/Cyber-Attack-Detector/blob/master/chapters/04-chat-memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iDOFtWugC8x"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/04-chat-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZdSobItaTI"
      },
      "source": [
        "### LangChain Essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGqZi42taTJ"
      },
      "source": [
        "# Conversational Memory for OpenAI - LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkg11xv9taTK"
      },
      "source": [
        "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
        "\n",
        "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "ydYy0Wc7tfLl"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3.3 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"pydantic==2.10.6\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K1xMfLPgO9f",
        "outputId": "a63c4568-6602-4ba1-f052-e74a50069936"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic==2.10.6 in /usr/local/lib/python3.12/dist-packages (2.10.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIJ-etvtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euKN4BNCtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "FSfe1JIwtaTK"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-chat-memory-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ57c5BxtaTL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYjdtcPtaTL"
      },
      "source": [
        "## LangChain's Memory Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czt55jO1taTL"
      },
      "source": [
        "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
        "\n",
        "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
        "\n",
        "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
        "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
        "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
        "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
        "\n",
        "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brmml7G-taTL"
      },
      "source": [
        "## Initialize our LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1vhQcsxtaTL"
      },
      "source": [
        "Before jumping into our memory types, let's initialize our LLM. We will use OpenAI's `gpt-4o-mini` model, if you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "bNMunYQZtaTL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENROUTER_API_KEY = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatOpenAI(temperature=0.0, api_key=OPENROUTER_API_KEY, base_url=\"https://openrouter.ai/api/v1\", model=\"x-ai/grok-4-fast:free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRVD0HctaTM"
      },
      "source": [
        "## 1. `ConversationBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40zJzXXXtaTM"
      },
      "source": [
        "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
        "\n",
        "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "tY-BxLW3taTM"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkCBXxitaTM"
      },
      "source": [
        "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
        "\n",
        "```\n",
        "User: Hi, my name is James\n",
        "AI: Hey James, what's up? I'm an AI model called Zeta.\n",
        "User: I'm researching the different types of conversational memory.\n",
        "AI: That's interesting, what are some examples?\n",
        "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "User: Buffer memory just stores the entire conversation, right?\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "User: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n",
        "\n",
        "We do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUPUE9DhtaTM"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is James\"},  # user message\n",
        "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6C4W54taTM"
      },
      "source": [
        "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRLwtZvWtaTM",
        "outputId": "0eef0be1-5d5c-4980-ffa3-327ea98f1cc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_qjbVustaTM"
      },
      "source": [
        "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjy4g_GtaTM",
        "outputId": "490b8cae-e952-4408-d339-c0baad5bc275"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1tkc6DOtaTM"
      },
      "source": [
        "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwgjzC0ktaTM",
        "outputId": "372e7dd2-15fc-4b4e-ef07-08a253dd73d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-839683240.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  chain = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1fMZLWtaTN",
        "outputId": "d2215fba-ae78-492d-a7f3-a4981d774164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Your name is James, as you mentioned right at the start of our chat. If that's not right or if you've got a nickname or something else in mind, feel free to correct me—I'm all ears! What's next on your mind about conversational memory?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Your name is James, as you mentioned right at the start of our chat. If that's not right or if you've got a nickname or something else in mind, feel free to correct me—I'm all ears! What's next on your mind about conversational memory?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQTzPaovtaTN"
      },
      "source": [
        "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZa82dhtaTN"
      },
      "source": [
        "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
        "\n",
        "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "UijZzCtutaTN"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEAavfntaTN"
      },
      "source": [
        "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "Ywxify04taTN"
      },
      "outputs": [],
      "source": [
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhOF-uVtaTN"
      },
      "source": [
        "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "BXKmcliGtaTN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39LBschEtaTN"
      },
      "source": [
        "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "zHQRgNfrtaTO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwEeNAVtaTO"
      },
      "source": [
        "Now we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESDoue1taTO",
        "outputId": "b407f737-d95d-49b8-ba61-8dbbe16618cf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 96, 'prompt_tokens': 130, 'total_tokens': 226, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 81, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-5c0ee9c7-ceaf-4aeb-9efd-6f8bb08e07cc-0', usage_metadata={'input_tokens': 130, 'output_tokens': 96, 'total_tokens': 226, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 81}})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "pipeline_with_history.invoke({\n",
        "    \"query\": \"Hi my name is James\"\n",
        "}, config={ \"session_id\": \"abc_123\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avL-i76_taTO"
      },
      "source": [
        "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgfzWIBstaTO",
        "outputId": "d1bf2ddb-7208-4e50-9f07-e2c23be8cc02"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James! 😊 What else can I assist you with?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 159, 'total_tokens': 247, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 73, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-323abd31-cfaf-4ca0-9ed9-4c7fb3f58761-0', usage_metadata={'input_tokens': 159, 'output_tokens': 88, 'total_tokens': 247, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 73}})"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"abc_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjSuam-taTO"
      },
      "source": [
        "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory` class. Let's continue onto other memory types and see how these can be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0MliXn-taTO"
      },
      "source": [
        "## 2. `ConversationBufferWindowMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lziswUkqtaTO"
      },
      "source": [
        "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
        "\n",
        "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
        "\n",
        "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
        "\n",
        "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
        "\n",
        "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBb46qDDtaTO",
        "outputId": "99e71fe7-48be-4579-d404-e5ec8c933e09"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-c190440c16c0>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6rLd_ataTO"
      },
      "source": [
        "We populate this memory using the same methods as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfxuLS7taTP",
        "outputId": "51772b66-bda9-406a-b6e3-d026c231f24f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_6vOOwtaTP"
      },
      "source": [
        "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "opmjk94ltaTP"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-g586vtaTP"
      },
      "source": [
        "Now let's see if our LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWa5mhzftaTP",
        "outputId": "19a63db6-82ff-4ba3-84de-e3d4c42f3ce7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"I'm sorry, but I don't have access to your name or any personal information. If you'd like to share it, I can remember it for the duration of our conversation!\"}"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GrFsMbStaTP"
      },
      "source": [
        "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
        "\n",
        "Based on the agent forgetting our name, we might wonder _why_ we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyXf5nKtaTP"
      },
      "source": [
        "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaUDp7GtaTP"
      },
      "source": [
        "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
        "\n",
        "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "fQUv8yPltaTP"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "        super().__init__(k=k)\n",
        "        print(f\"Initializing BufferWindowMessageHistory with k={k}\")\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "uoDJ38vztaTP"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "    print(f\"get_chat_history called with session_id={session_id} and k={k}\")\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "    # remove anything beyond the last\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "GsXhm811taTP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQR8TZLGtaTQ"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `k` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBCnrghLtaTQ",
        "outputId": "182ecc51-d3f3-496f-e08a-aac424256146"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Nice to meet you, James! I'll remember that. How can I help you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 215, 'prompt_tokens': 205, 'total_tokens': 420, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 197, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-92b7b739-bf51-474b-8c49-3cae920c3623-0', usage_metadata={'input_tokens': 205, 'output_tokens': 215, 'total_tokens': 420, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 197}})"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AvqsaA3taTQ"
      },
      "source": [
        "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDMrCS26taTQ",
        "outputId": "5f0cc965-d596-4e8d-c8b7-814fe012fdad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "chat_map[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k4\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzArAVDtaTQ"
      },
      "source": [
        "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn3K6fxtaTQ",
        "outputId": "7cc0faed-ecf5-46ca-a41d-c84eb9968680"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k4 and k=4\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your name is James! How's it going?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 213, 'total_tokens': 369, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 146, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-d8cc101f-3e16-46f7-a435-dec903ddf09a-0', usage_metadata={'input_tokens': 213, 'output_tokens': 156, 'total_tokens': 369, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 146}})"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN7am67YtaTQ"
      },
      "source": [
        "Now let's initialize a new session with `k=14`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXU6nI0taTQ",
        "outputId": "af5a92b3-b72e-491b-e5a8-85a209b4e5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! Good to see you back. Still diving into those conversational memory types, or got something new on your mind?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 235, 'prompt_tokens': 299, 'total_tokens': 534, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 210, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-bd47b4bd-62c2-40f8-9343-57013b4ca4ef-0', usage_metadata={'input_tokens': 299, 'output_tokens': 235, 'total_tokens': 534, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 210}})"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPyLeEOtaTQ"
      },
      "source": [
        "We'll manually insert the remaining messages as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyHMnkCtaTQ",
        "outputId": "ddf7ed06-fb40-4b4a-9102-394ecc297f05"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Your name is James! How's the research going?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 274, 'total_tokens': 398, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 113, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-be01935c-8a40-461a-8065-46da824f4b36-0', usage_metadata={'input_tokens': 274, 'output_tokens': 124, 'total_tokens': 398, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 113}}),\n",
              " HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Good to see you back. Still diving into those conversational memory types, or got something new on your mind?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 235, 'prompt_tokens': 299, 'total_tokens': 534, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 210, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-bd47b4bd-62c2-40f8-9343-57013b4ca4ef-0', usage_metadata={'input_tokens': 299, 'output_tokens': 235, 'total_tokens': 534, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 210}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8X1xV9ItaTQ",
        "outputId": "30f1e0e6-b191-454c-f0d2-0fc2d3f5d35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "get_chat_history called with session_id=id_k14 and k=14\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your name is James! How's the research going?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 352, 'prompt_tokens': 325, 'total_tokens': 677, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 341, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 154}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-ddf0bd71-5e4b-4e29-9c15-2ebadc7ad73a-0', usage_metadata={'input_tokens': 325, 'output_tokens': 352, 'total_tokens': 677, 'input_token_details': {'audio': 0, 'cache_read': 154}, 'output_token_details': {'reasoning': 341}})"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3SqKjEvtaTQ",
        "outputId": "c962df5a-29dd-4a79-e325-b58e036bcc76"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Your name is James! How's the research going?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 274, 'total_tokens': 398, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 113, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-be01935c-8a40-461a-8065-46da824f4b36-0', usage_metadata={'input_tokens': 274, 'output_tokens': 124, 'total_tokens': 398, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 113}}),\n",
              " HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Good to see you back. Still diving into those conversational memory types, or got something new on your mind?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 235, 'prompt_tokens': 299, 'total_tokens': 534, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 210, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-bd47b4bd-62c2-40f8-9343-57013b4ca4ef-0', usage_metadata={'input_tokens': 299, 'output_tokens': 235, 'total_tokens': 534, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 210}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Your name is James! How's the research going?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 352, 'prompt_tokens': 325, 'total_tokens': 677, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 341, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 154}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-ddf0bd71-5e4b-4e29-9c15-2ebadc7ad73a-0', usage_metadata={'input_tokens': 325, 'output_tokens': 352, 'total_tokens': 677, 'input_token_details': {'audio': 0, 'cache_read': 154}, 'output_token_details': {'reasoning': 341}})]"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91uVCqFtaTQ"
      },
      "source": [
        "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK51EzantaTR"
      },
      "source": [
        "## 3. `ConversationSummaryMemory`\n",
        "\n",
        "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
        "\n",
        "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "m1DRv1umtaTR"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDK1xTiQtaTR"
      },
      "source": [
        "Unlike with the previous memory types, we need to provide an `llm` to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
        "\n",
        "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "A5hxfssAtaTR"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nHb-SqtaTR"
      },
      "source": [
        "Let's test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwCtQA0RtaTR",
        "outputId": "3b3e0a80-a382-492d-987d-3f3bbdbf7a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hello there my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI, describes its design as helpful, truthful, and witty, notes its knowledge base up to 2023 covering various topics, and asks what's on James's mind, including hobbies or questions.\n",
            "Human: I am researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI, describes its design as helpful, truthful, and witty, notes its knowledge base up to 2023 covering various topics, and asks what's on James's mind, including hobbies or questions. James mentions researching different types of conversational memory. Grok provides a detailed overview of short-term (context window-based, temporary), long-term (persistent via databases/RAG for continuity), and hybrid/multi-level (combining both with buffers/entities) memory systems, including how they work, pros/cons, examples from models like GPT-4 and BlenderBot, and research references up to 2023; Grok asks what sparked James's interest in the topic.\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI, describes its design as helpful, truthful, and witty, notes its knowledge base up to 2023 covering various topics, and asks what's on James's mind, including hobbies or questions. James mentions researching different types of conversational memory. Grok provides a detailed overview of short-term (context window-based, temporary), long-term (persistent via databases/RAG for continuity), and hybrid/multi-level (combining both with buffers/entities) memory systems, including how they work, pros/cons, examples from models like GPT-4 and BlenderBot, and research references up to 2023; Grok asks what sparked James's interest in the topic. James specifies interest in ConversationBufferMemory and ConversationBufferWindowMemory from the LangChain framework. Grok explains these as short-term memory implementations: ConversationBufferMemory stores full conversation history in a simple buffer for complete recall but risks token limits and inefficiency; ConversationBufferWindowMemory limits to the most recent k exchanges (e.g., FIFO queue) for better scalability and recency focus but may lose older context; includes pseudocode examples, pros/cons (e.g., simplicity vs. scalability), use cases (e.g., chatbots, virtual assistants), ties to research (e.g., Longformer, BlenderBot), and comparisons (full vs. pruned history, combinable with long-term stores); Grok asks what specifically sparked interest in these, if for a project or research, and offers to brainstorm.\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "New summary:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI, describes its design as helpful, truthful, and witty, notes its knowledge base up to 2023 covering various topics, and asks what's on James's mind, including hobbies or questions. James mentions researching different types of conversational memory. Grok provides a detailed overview of short-term (context window-based, temporary), long-term (persistent via databases/RAG for continuity), and hybrid/multi-level (combining both with buffers/entities) memory systems, including how they work, pros/cons, examples from models like GPT-4 and BlenderBot, and research references up to 2023; Grok asks what sparked James's interest in the topic. James specifies interest in ConversationBufferMemory and ConversationBufferWindowMemory from the LangChain framework. Grok explains these as short-term memory implementations: ConversationBufferMemory stores full conversation history in a simple buffer for complete recall but risks token limits and inefficiency; ConversationBufferWindowMemory limits to the most recent k exchanges (e.g., FIFO queue) for better scalability and recency focus but may lose older context; includes pseudocode examples, pros/cons (e.g., simplicity vs. scalability), use cases (e.g., chatbots, virtual assistants), ties to research (e.g., Longformer, BlenderBot), and comparisons (full vs. pruned history, combinable with long-term stores); Grok asks what specifically sparked interest in these, if for a project or research, and offers to brainstorm. James simplifies that Buffer memory just stores the entire conversation. Grok confirms this, providing a deeper breakdown of ConversationBufferMemory's mechanism (e.g., appending messages to a list, passing full history to LLM, pseudocode example), pros (simplicity, full recall, low overhead for short sessions), cons (token limits, inefficiency, scalability issues for long chats), ties to research (e.g., early chatbots like ELIZA, memory-augmented networks), and contrasts with ConversationBufferWindowMemory (unlimited vs. sliding window of last k exchanges); Grok asks if this aligns with James's experiments, any pain points like token overflow, and offers code snippets or integration ideas for projects.\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
              " 'history': \"New summary:\\nThe human introduces himself as James. The AI introduces itself as Grok, built by xAI, describes its design as helpful, truthful, and witty, notes its knowledge base up to 2023 covering various topics, and asks what's on James's mind, including hobbies or questions. James mentions researching different types of conversational memory. Grok provides a detailed overview of short-term (context window-based, temporary), long-term (persistent via databases/RAG for continuity), and hybrid/multi-level (combining both with buffers/entities) memory systems, including how they work, pros/cons, examples from models like GPT-4 and BlenderBot, and research references up to 2023; Grok asks what sparked James's interest in the topic. James specifies interest in ConversationBufferMemory and ConversationBufferWindowMemory from the LangChain framework. Grok explains these as short-term memory implementations: ConversationBufferMemory stores full conversation history in a simple buffer for complete recall but risks token limits and inefficiency; ConversationBufferWindowMemory limits to the most recent k exchanges (e.g., FIFO queue) for better scalability and recency focus but may lose older context; includes pseudocode examples, pros/cons (e.g., simplicity vs. scalability), use cases (e.g., chatbots, virtual assistants), ties to research (e.g., Longformer, BlenderBot), and comparisons (full vs. pruned history, combinable with long-term stores); Grok asks what specifically sparked interest in these, if for a project or research, and offers to brainstorm. James simplifies that Buffer memory just stores the entire conversation. Grok confirms this, providing a deeper breakdown of ConversationBufferMemory's mechanism (e.g., appending messages to a list, passing full history to LLM, pseudocode example), pros (simplicity, full recall, low overhead for short sessions), cons (token limits, inefficiency, scalability issues for long chats), ties to research (e.g., early chatbots like ELIZA, memory-augmented networks), and contrasts with ConversationBufferWindowMemory (unlimited vs. sliding window of last k exchanges); Grok asks if this aligns with James's experiments, any pain points like token overflow, and offers code snippets or integration ideas for projects.\",\n",
              " 'response': 'That\\'s a spot-on simplification, James—ConversationBufferWindowMemory in LangChain does indeed act like a sliding window that keeps only the most recent k messages (or exchanges) in the buffer, systematically dropping older ones to maintain efficiency. It\\'s a clever way to handle the trade-offs in conversational AI without letting the history balloon out of control. Let me expand on this a bit more, building on what we\\'ve discussed about the full buffer versus this windowed approach, and I\\'ll throw in some practical details to make it clearer.\\n\\n### How It Works Under the Hood\\nAt its core, ConversationBufferWindowMemory uses a queue-like structure (often implemented as a list or deque in Python) to store conversation turns. Each \"message\" typically includes the role (human or AI) and the content, and the window size k is configurable—say, k=5 for the last 5 full exchanges (human + AI response) or sometimes just the last k human inputs, depending on your setup.\\n\\nHere\\'s a quick pseudocode breakdown to illustrate (inspired by LangChain\\'s actual implementation up to 2023):\\n\\n```python\\nclass ConversationBufferWindowMemory:\\n    def __init__(self, k=5):  # Default window size\\n        self.chat_memory = []  # List to hold message dicts: [{\\'role\\': \\'human\\', \\'content\\': \\'Hi\\'}, {\\'role\\': \\'ai\\', \\'content\\': \\'Hello!\\'}]\\n        self.k = k\\n    \\n    def save_context(self, inputs, outputs):\\n        # Append the new human input and AI output to the buffer\\n        self.chat_memory.append({\\'role\\': \\'human\\', \\'content\\': inputs[\\'input\\']})\\n        self.chat_memory.append({\\'role\\': \\'ai\\', \\'content\\': outputs[\\'output\\']})\\n        \\n        # Trim the buffer to the last k messages if it exceeds\\n        if len(self.chat_memory) > self.k:\\n            self.chat_memory = self.chat_memory[-self.k:]  # Keep only the most recent k (FIFO drop of oldest)\\n    \\n    def load_memory_variables(self):\\n        # Return the current buffer as a formatted string for the LLM prompt\\n        history = \"\"\\n        for msg in self.chat_memory:\\n            history += f\"{msg[\\'role\\'].title()}: {msg[\\'content\\']}\\\\n\"\\n        return {\\'history\\': history}\\n```\\n\\nWhen you pass this to a language model (like via LangChain\\'s chain), it injects only this pruned history into the prompt, so the LLM gets recency-biased context without the full backlog. For example, if k=3 and your conversation has 10 exchanges, it drops the first 7, focusing on the latest 3 to keep things snappy and under token limits (e.g., GPT-3.5\\'s 4K token cap or GPT-4\\'s 8K/32K variants as of 2023).\\n\\n### Pros and Cons in Practice\\n- **Pros**:\\n  - **Scalability**: Ideal for long-running chats where full history would hit token ceilings or slow down inference. It prevents \"context dilution\" by prioritizing what\\'s fresh and relevant.\\n  - **Efficiency**: Lower computational overhead—fewer tokens mean faster responses and cheaper API calls. Great for real-time apps like customer support bots.\\n  - **Recency Focus**: Mimics human conversation, where we often forget distant details unless they\\'re crucial. Research from models like BlenderBot (Facebook AI, 2020) showed this boosts coherence in multi-turn dialogues.\\n\\n- **Cons**:\\n  - **Loss of Older Context**: If a user circles back to an early topic (e.g., \"Remember what I said about my hobby from 20 messages ago?\"), it might not recall, leading to frustrating \"I don\\'t remember that\" moments. This is a common pain point in prototypes.\\n  - **Tuning Challenges**: Picking the right k is key—too small (k=2), and it feels forgetful; too large (k=20), and you\\'re back to buffer-like issues. It doesn\\'t handle semantic importance well on its own (e.g., dropping a key fact just because it\\'s old).\\n  - **No Built-in Persistence**: Like the plain buffer, it\\'s in-memory and session-bound unless you pair it with a vector store for long-term recall.\\n\\n### Real-World Ties and Comparisons\\nThis windowed approach draws from earlier research, like the sliding window attention in transformers (e.g., Longformer, 2020, which efficiently handles long sequences by attending only to recent tokens). In LangChain ecosystems, it\\'s often combined with other tools—for instance, using it short-term while offloading summaries to a long-term memory like ConversationSummaryMemory or a RAG (Retrieval-Augmented Generation) setup with FAISS or Pinecone for querying past highlights.\\n\\nCompared to the full ConversationBufferMemory we talked about last time, the window is like a \"pruned archive\": unlimited storage becomes a fixed-size FIFO queue. If you\\'re building something, you could even hybridize them—use the window for quick prompts and periodically summarize/dump to a persistent store.\\n\\nDoes this match what you\\'re seeing in your experiments with LangChain, James? Are you running into issues like choosing the optimal k value, or maybe integrating it with a specific LLM? If it\\'s for a project, I\\'d love to hear more—I could sketch a full code example or suggest tweaks for edge cases like handling variable-length messages. What\\'s next on your mind?'}"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"hello there my name is James\"})\n",
        "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
        "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
        "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
        "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgmdXd2taTR"
      },
      "source": [
        "We can see how the conversation summary varies with each new message. Let's see if the LLM is able to recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHnh4Y1OtaTR",
        "outputId": "f7b84230-5449-44fd-bfed-db138d9e2372"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI, describes its design as helpful, truthful, and witty, notes its knowledge base up to 2023 covering various topics, and asks what's on James's mind, including hobbies or questions. James mentions researching different types of conversational memory. Grok provides a detailed overview of short-term (context window-based, temporary), long-term (persistent via databases/RAG for continuity), and hybrid/multi-level (combining both with buffers/entities) memory systems, including how they work, pros/cons, examples from models like GPT-4 and BlenderBot, and research references up to 2023; Grok asks what sparked James's interest in the topic. James specifies interest in ConversationBufferMemory and ConversationBufferWindowMemory from the LangChain framework. Grok explains these as short-term memory implementations: ConversationBufferMemory stores full conversation history in a simple buffer for complete recall but risks token limits and inefficiency; ConversationBufferWindowMemory limits to the most recent k exchanges (e.g., FIFO queue) for better scalability and recency focus but may lose older context; includes pseudocode examples, pros/cons (e.g., simplicity vs. scalability), use cases (e.g., chatbots, virtual assistants), ties to research (e.g., Longformer, BlenderBot), and comparisons (full vs. pruned history, combinable with long-term stores); Grok asks what specifically sparked interest in these, if for a project or research, and offers to brainstorm. James simplifies that Buffer memory just stores the entire conversation. Grok confirms this, providing a deeper breakdown of ConversationBufferMemory's mechanism (e.g., appending messages to a list, passing full history to LLM, pseudocode example), pros (simplicity, full recall, low overhead for short sessions), cons (token limits, inefficiency, scalability issues for long chats), ties to research (e.g., early chatbots like ELIZA, memory-augmented networks), and contrasts with ConversationBufferWindowMemory (unlimited vs. sliding window of last k exchanges); Grok asks if this aligns with James's experiments, any pain points like token overflow, and offers code snippets or integration ideas for projects. James further simplifies that BufferWindowMemory stores the last k messages, dropping the rest. Grok confirms this, expanding on its mechanics (queue-like structure for recent exchanges, configurable k, pseudocode example showing append/trim/load for LLM prompts), pros (scalability, efficiency, recency focus, ties to BlenderBot/Longformer research), cons (loss of older context, tuning challenges, no persistence), real-world ties (e.g., transformer sliding windows, hybrid with RAG/summary memory), and comparisons to full buffer (pruned FIFO vs. unlimited); Grok asks if this matches James's experiments, issues like optimal k or LLM integration, and offers code sketches or project tweaks.\n",
            "Human: What is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my name again?',\n",
              " 'history': \"The human introduces himself as James. The AI introduces itself as Grok, built by xAI, describes its design as helpful, truthful, and witty, notes its knowledge base up to 2023 covering various topics, and asks what's on James's mind, including hobbies or questions. James mentions researching different types of conversational memory. Grok provides a detailed overview of short-term (context window-based, temporary), long-term (persistent via databases/RAG for continuity), and hybrid/multi-level (combining both with buffers/entities) memory systems, including how they work, pros/cons, examples from models like GPT-4 and BlenderBot, and research references up to 2023; Grok asks what sparked James's interest in the topic. James specifies interest in ConversationBufferMemory and ConversationBufferWindowMemory from the LangChain framework. Grok explains these as short-term memory implementations: ConversationBufferMemory stores full conversation history in a simple buffer for complete recall but risks token limits and inefficiency; ConversationBufferWindowMemory limits to the most recent k exchanges (e.g., FIFO queue) for better scalability and recency focus but may lose older context; includes pseudocode examples, pros/cons (e.g., simplicity vs. scalability), use cases (e.g., chatbots, virtual assistants), ties to research (e.g., Longformer, BlenderBot), and comparisons (full vs. pruned history, combinable with long-term stores); Grok asks what specifically sparked interest in these, if for a project or research, and offers to brainstorm. James simplifies that Buffer memory just stores the entire conversation. Grok confirms this, providing a deeper breakdown of ConversationBufferMemory's mechanism (e.g., appending messages to a list, passing full history to LLM, pseudocode example), pros (simplicity, full recall, low overhead for short sessions), cons (token limits, inefficiency, scalability issues for long chats), ties to research (e.g., early chatbots like ELIZA, memory-augmented networks), and contrasts with ConversationBufferWindowMemory (unlimited vs. sliding window of last k exchanges); Grok asks if this aligns with James's experiments, any pain points like token overflow, and offers code snippets or integration ideas for projects. James further simplifies that BufferWindowMemory stores the last k messages, dropping the rest. Grok confirms this, expanding on its mechanics (queue-like structure for recent exchanges, configurable k, pseudocode example showing append/trim/load for LLM prompts), pros (scalability, efficiency, recency focus, ties to BlenderBot/Longformer research), cons (loss of older context, tuning challenges, no persistence), real-world ties (e.g., transformer sliding windows, hybrid with RAG/summary memory), and comparisons to full buffer (pruned FIFO vs. unlimited); Grok asks if this matches James's experiments, issues like optimal k or LLM integration, and offers code sketches or project tweaks.\",\n",
              " 'response': \"Ah, James! That's your name—you introduced yourself right at the start of our chat. I remember it clearly from our conversation history, where you kicked things off by saying hello as James. My memory setup (drawing from that short-term buffer we've been discussing) keeps track of these details without a hitch, at least for sessions like this. No token overflow or dropped exchanges here yet!\\n\\nIf I've got that wrong somehow, feel free to correct me—I'm all ears (or algorithms, as it were). What's next on your mind? Still pondering those LangChain memory types, or something new?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"What is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZpfPyEztaTR"
      },
      "source": [
        "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlKvJDUtaTR"
      },
      "source": [
        "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9jdJCkztaTR"
      },
      "source": [
        "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "C09U1WkqtaTR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages.content,\n",
        "                messages=[x.content for x in messages]\n",
        "            )\n",
        "        )\n",
        "        # replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "762Vg5u4taTR"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "om78sKCGtaTS"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPfut3S5taTS"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1F1m9n2taTS",
        "outputId": "61fd5151-d5bc-41d0-8661-50a0b36cda87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a5ffaf0-771f-4a62-9549-cc9682d94f81-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 83}})"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqOCHCTtaTS"
      },
      "source": [
        "Let's see what summary was generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDeudJftaTS",
        "outputId": "939a3b63-d88c-4a04-b732-190053607ec8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a5ffaf0-771f-4a62-9549-cc9682d94f81-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 83}})]"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uLeGXztaTS"
      },
      "source": [
        "Let's continue the conversation and see if the summary is updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7WVWmCtaTS",
        "outputId": "f18c0475-0b35-4d90-d405-74afb5924a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a5ffaf0-771f-4a62-9549-cc9682d94f81-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 83}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s a fascinating topic! Conversational memory refers to how systems (like AI chatbots or humans) retain, recall, and use information from past interactions to make dialogues more coherent and context-aware. It draws from fields like AI, cognitive psychology, and linguistics. I\\'ll break down the main types of conversational memory below, focusing on both human and AI perspectives for a well-rounded view. If you\\'re leaning toward a specific angle (e.g., AI implementation or psychological models), let me know!\\n\\n### 1. **Short-Term (or Working) Memory**\\n   - **Description**: This is the temporary storage of information relevant to the immediate conversation. It holds details from the recent exchange but fades quickly without reinforcement.\\n   - **In Humans**: Based on models like Baddeley\\'s working memory framework, it includes a \"phonological loop\" for verbal info and a \"visuospatial sketchpad\" for non-verbal cues. Capacity is limited (about 7±2 items, per Miller\\'s Law).\\n   - **In AI**: Often tied to the model\\'s context window (e.g., in GPT models, this can be thousands of tokens). It keeps track of the ongoing chat but resets between sessions unless extended.\\n   - **Example**: In a chat, remembering the last few sentences to respond appropriately, like recalling you said \"Hi, my name is James\" just moments ago.\\n   - **Strengths/Limitations**: Fast and efficient but prone to overload or forgetting if the conversation gets too long.\\n\\n### 2. **Long-Term Memory**\\n   - **Description**: Persistent storage of information from past conversations, allowing recall over time, sessions, or even indefinitely.\\n   - **Subtypes**:\\n     - **Episodic Memory**: Stores specific events or interactions with context (who, what, when). It\\'s autobiographical and time-stamped.\\n       - Human Example: Recalling a fun debate you had with a friend last week.\\n       - AI Example: A chatbot like me remembering your previous query about conversational memory in a future session (if the system supports it).\\n     - **Semantic Memory**: Stores general facts, patterns, or knowledge extracted from conversations, without tying to specific events.\\n       - Human Example: Knowing that \"conversational memory\" relates to AI after reading about it multiple times.\\n       - AI Example: Learning user preferences, like your interest in research topics, to tailor responses.\\n     - **Procedural Memory**: Implicit knowledge of how to conduct conversations (e.g., turn-taking, politeness norms). This is more \"how-to\" than \"what.\"\\n   - **In AI**: Implemented via databases, vector embeddings (e.g., in retrieval-augmented generation or RAG systems), or fine-tuning. Tools like LangChain or Pinecone help manage this.\\n   - **Strengths/Limitations**: Builds continuity and personalization but can lead to privacy issues or outdated info if not updated.\\n\\n### 3. **Contextual or Session-Based Memory**\\n   - **Description**: A hybrid that maintains state within a single conversation or session but doesn\\'t persist long-term. It\\'s about threading the dialogue logically.\\n   - **In Humans**: Relies on cues like shared references (e.g., \"Remember that story I told you?\") to jog recall.\\n   - **In AI**: Uses conversation history buffers or state machines. For instance, in multi-turn dialogues, the model references prior messages to avoid repetition.\\n   - **Example**: In this chat, I\\'m using contextual memory to know you\\'re researching this topic and build on your intro.\\n   - **Strengths/Limitations**: Enhances flow but can break if the session ends abruptly.\\n\\n### 4. **External or Augmented Memory**\\n   - **Description**: Offloads memory to external tools, like notes, databases, or search engines, to extend capacity beyond internal limits.\\n   - **In Humans**: Using a notebook or phone reminders during talks.\\n   - **In AI**: Integrates with APIs (e.g., searching the web for facts) or memory stores like Redis. Emerging tech like \"memory graphs\" in AI agents (e.g., in AutoGPT) links related concepts dynamically.\\n   - **Example**: An AI pulling up a user\\'s past emails to reference in a support chat.\\n   - **Strengths/Limitations**: Scalable and accurate but depends on reliable external systems.\\n\\n### Key Challenges and Trends\\n- **Challenges**: Forgetting (catastrophic forgetting in AI training), privacy (storing personal data), and bias (recalling skewed info).\\n- **Trends**: In AI, we\\'re seeing advances like infinite context windows (e.g., via sparse attention) and hybrid human-AI memory systems. Research in NLP (natural language processing) often cites models like those from OpenAI or Google for inspiration.\\n\\nIf you\\'d like resources (books like \"The Psychology of Conversation\" or papers on arXiv), examples from real AI systems, or how this applies to a specific use case, just say the word. What\\'s sparking your interest in this?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1184, 'prompt_tokens': 163, 'total_tokens': 1347, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 176, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 138}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-873c13e6-6a93-472b-9cdf-fe2027b404e4-0', usage_metadata={'input_tokens': 163, 'output_tokens': 1184, 'total_tokens': 1347, 'input_token_details': {'audio': 0, 'cache_read': 138}, 'output_token_details': {'reasoning': 176}})]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqEv8kxZtaTS"
      },
      "source": [
        "So far so good! Let's continue with a few more messages before returning to the name question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "F7cy8dKZtaTS",
        "outputId": "a5ce2759-4006-453d-b170-2438c186fd3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOdFy4htaTS"
      },
      "source": [
        "Let's see the latest summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEBczVutaTS",
        "outputId": "3104b9a0-5ddb-4be3-c0a5-01acc9c1fe6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a5ffaf0-771f-4a62-9549-cc9682d94f81-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 83}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s a fascinating topic! Conversational memory refers to how systems (like AI chatbots or humans) retain, recall, and use information from past interactions to make dialogues more coherent and context-aware. It draws from fields like AI, cognitive psychology, and linguistics. I\\'ll break down the main types of conversational memory below, focusing on both human and AI perspectives for a well-rounded view. If you\\'re leaning toward a specific angle (e.g., AI implementation or psychological models), let me know!\\n\\n### 1. **Short-Term (or Working) Memory**\\n   - **Description**: This is the temporary storage of information relevant to the immediate conversation. It holds details from the recent exchange but fades quickly without reinforcement.\\n   - **In Humans**: Based on models like Baddeley\\'s working memory framework, it includes a \"phonological loop\" for verbal info and a \"visuospatial sketchpad\" for non-verbal cues. Capacity is limited (about 7±2 items, per Miller\\'s Law).\\n   - **In AI**: Often tied to the model\\'s context window (e.g., in GPT models, this can be thousands of tokens). It keeps track of the ongoing chat but resets between sessions unless extended.\\n   - **Example**: In a chat, remembering the last few sentences to respond appropriately, like recalling you said \"Hi, my name is James\" just moments ago.\\n   - **Strengths/Limitations**: Fast and efficient but prone to overload or forgetting if the conversation gets too long.\\n\\n### 2. **Long-Term Memory**\\n   - **Description**: Persistent storage of information from past conversations, allowing recall over time, sessions, or even indefinitely.\\n   - **Subtypes**:\\n     - **Episodic Memory**: Stores specific events or interactions with context (who, what, when). It\\'s autobiographical and time-stamped.\\n       - Human Example: Recalling a fun debate you had with a friend last week.\\n       - AI Example: A chatbot like me remembering your previous query about conversational memory in a future session (if the system supports it).\\n     - **Semantic Memory**: Stores general facts, patterns, or knowledge extracted from conversations, without tying to specific events.\\n       - Human Example: Knowing that \"conversational memory\" relates to AI after reading about it multiple times.\\n       - AI Example: Learning user preferences, like your interest in research topics, to tailor responses.\\n     - **Procedural Memory**: Implicit knowledge of how to conduct conversations (e.g., turn-taking, politeness norms). This is more \"how-to\" than \"what.\"\\n   - **In AI**: Implemented via databases, vector embeddings (e.g., in retrieval-augmented generation or RAG systems), or fine-tuning. Tools like LangChain or Pinecone help manage this.\\n   - **Strengths/Limitations**: Builds continuity and personalization but can lead to privacy issues or outdated info if not updated.\\n\\n### 3. **Contextual or Session-Based Memory**\\n   - **Description**: A hybrid that maintains state within a single conversation or session but doesn\\'t persist long-term. It\\'s about threading the dialogue logically.\\n   - **In Humans**: Relies on cues like shared references (e.g., \"Remember that story I told you?\") to jog recall.\\n   - **In AI**: Uses conversation history buffers or state machines. For instance, in multi-turn dialogues, the model references prior messages to avoid repetition.\\n   - **Example**: In this chat, I\\'m using contextual memory to know you\\'re researching this topic and build on your intro.\\n   - **Strengths/Limitations**: Enhances flow but can break if the session ends abruptly.\\n\\n### 4. **External or Augmented Memory**\\n   - **Description**: Offloads memory to external tools, like notes, databases, or search engines, to extend capacity beyond internal limits.\\n   - **In Humans**: Using a notebook or phone reminders during talks.\\n   - **In AI**: Integrates with APIs (e.g., searching the web for facts) or memory stores like Redis. Emerging tech like \"memory graphs\" in AI agents (e.g., in AutoGPT) links related concepts dynamically.\\n   - **Example**: An AI pulling up a user\\'s past emails to reference in a support chat.\\n   - **Strengths/Limitations**: Scalable and accurate but depends on reliable external systems.\\n\\n### Key Challenges and Trends\\n- **Challenges**: Forgetting (catastrophic forgetting in AI training), privacy (storing personal data), and bias (recalling skewed info).\\n- **Trends**: In AI, we\\'re seeing advances like infinite context windows (e.g., via sparse attention) and hybrid human-AI memory systems. Research in NLP (natural language processing) often cites models like those from OpenAI or Google for inspiration.\\n\\nIf you\\'d like resources (books like \"The Psychology of Conversation\" or papers on arXiv), examples from real AI systems, or how this applies to a specific use case, just say the word. What\\'s sparking your interest in this?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1184, 'prompt_tokens': 163, 'total_tokens': 1347, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 176, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 138}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-873c13e6-6a93-472b-9cdf-fe2027b404e4-0', usage_metadata={'input_tokens': 163, 'output_tokens': 1184, 'total_tokens': 1347, 'input_token_details': {'audio': 0, 'cache_read': 138}, 'output_token_details': {'reasoning': 176}}),\n",
              " HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s great—diving into LangChain\\'s memory implementations is a smart way to explore practical conversational memory in AI systems. ConversationBufferMemory and ConversationBufferWindowMemory are two foundational classes in LangChain (a Python framework for LLM chains and agents) designed to handle chat history for more context-aware interactions. They both store conversation data as a simple buffer of messages but differ in how they manage size and retention. I\\'ll explain each, compare them, and touch on implementation basics. If you\\'re coding this or need examples, let me know!\\n\\n### 1. **ConversationBufferMemory**\\n   - **Description**: This is the simplest form of memory in LangChain. It acts like an unbounded buffer that stores *every* message in the conversation history, from the very first exchange onward. Each new human or AI message is appended to the buffer, and the full history is passed to the LLM (large language model) with every subsequent call.\\n   - **How It Works**:\\n     - Messages are stored as a list of `HumanMessage` and `AIMessage` objects (or similar, depending on the chat model).\\n     - When generating a response, the entire buffer is injected into the prompt as context (e.g., via a template like \"Human: {input}\\\\nAI: {history}\\\\nHuman:\").\\n     - No automatic pruning—it\\'s \"infinite\" until you manually clear or limit it.\\n   - **Use Cases**:\\n     - Short, focused conversations where full history is valuable and token limits aren\\'t a concern (e.g., a quick Q&A bot).\\n     - Prototyping or debugging, as it preserves everything for easy inspection.\\n   - **Pros**:\\n     - Simple to implement and understand.\\n     - Maintains complete context, reducing \"forgetfulness\" in ongoing dialogues.\\n   - **Cons**:\\n     - Can quickly exceed LLM context windows (e.g., 4K–128K tokens in models like GPT-3.5 or GPT-4), leading to high costs, slower responses, or truncation.\\n     - Not scalable for long-term or high-volume chats—performance degrades as history grows.\\n   - **Basic Implementation Example (Python with LangChain)**:\\n     ```python\\n     from langchain.memory import ConversationBufferMemory\\n     from langchain.chains import ConversationChain\\n     from langchain.llms import OpenAI  # Or your preferred LLM\\n\\n     llm = OpenAI(temperature=0)\\n     memory = ConversationBufferMemory()\\n     conversation = ConversationChain(llm=llm, memory=memory)\\n\\n     # First interaction\\n     response1 = conversation.predict(input=\"Hi, I\\'m researching memory.\")\\n     print(response1)  # AI responds\\n\\n     # Second interaction—full history is used\\n     response2 = conversation.predict(input=\"Tell me about buffers.\")\\n     print(response2)\\n     print(memory.buffer)  # Shows full chat history\\n     ```\\n     Here, `memory.buffer` holds the entire transcript.\\n\\n### 2. **ConversationBufferWindowMemory**\\n   - **Description**: This builds on the buffer concept but adds a \"sliding window\" mechanism to limit memory to the most recent *k* interactions (where k is a configurable parameter, defaulting to 2). Older messages are automatically dropped as the conversation progresses, keeping the buffer size manageable.\\n   - **How It Works**:\\n     - Like the buffer memory, it stores messages in a list, but only the last k exchanges (human + AI pairs) are retained.\\n     - For example, if k=5, it keeps the last 5 human messages and their corresponding AI responses.\\n     - The window \"slides\" forward with each new message, pruning from the beginning.\\n     - You can also specify a `k` value and optionally return only messages (not full history) for efficiency.\\n   - **Use Cases**:\\n     - Medium-length conversations where recent context matters most (e.g., customer support bots handling sequential queries without needing ancient history).\\n     - Resource-constrained environments, like mobile apps or edge devices, to avoid token bloat.\\n   - **Pros**:\\n     - Balances context retention with efficiency—prevents context windows from overflowing.\\n     - More performant and cost-effective for longer sessions.\\n     - Easy to tune via the `k` parameter (e.g., set to 10 for more recall).\\n   - **Cons**:\\n     - Loses older context, which can cause \"amnesia\" if a user references early details (e.g., \"Remember what I said about James?\").\\n     - Requires careful choice of k; too small, and responses feel disjointed; too large, and it mimics the full buffer.\\n   - **Basic Implementation Example (Python with LangChain)**:\\n     ```python\\n     from langchain.memory import ConversationBufferWindowMemory\\n     from langchain.chains import ConversationChain\\n     from langchain.llms import OpenAI\\n\\n     llm = OpenAI(temperature=0)\\n     memory = ConversationBufferWindowMemory(k=2)  # Keep last 2 exchanges\\n     conversation = ConversationChain(llm=llm, memory=memory)\\n\\n     # Interactions—after 3+ exchanges, oldest are dropped\\n     response1 = conversation.predict(input=\"Hi, I\\'m James.\")\\n     response2 = conversation.predict(input=\"What\\'s short-term memory?\")\\n     response3 = conversation.predict(input=\"Now explain long-term.\")  # Only last 2 used\\n     print(response3)\\n     print(memory.buffer)  # Shows only recent history\\n     ```\\n\\n### Comparison and When to Choose\\n- **Similarities**: Both are lightweight, human-readable (store plain message lists), and integrate seamlessly with LangChain chains/agents. They\\'re great for basic stateful conversations without needing complex databases.\\n- **Key Differences**:\\n  | Aspect              | ConversationBufferMemory | ConversationBufferWindowMemory |\\n  |---------------------|---------------------------|--------------------------------|\\n  | **History Retention** | Full (unlimited)         | Limited to last k exchanges   |\\n  | **Scalability**     | Poor for long chats      | Better; controls token usage  |\\n  | **Complexity**      | Very simple              | Slightly more (tune k)        |\\n  | **Best For**        | Short, complete recall   | Ongoing, efficient dialogues  |\\n- **Trade-offs**: If your app needs persistent, long-term recall (e.g., across sessions), consider upgrading to something like `ConversationSummaryMemory` (which summarizes history) or `ConversationEntityMemory` (extracts key entities). For production, pair these with vector stores for semantic search.\\n- **Limitations in Practice**: These are in-memory only, so they don\\'t persist across restarts. For durability, use `ConversationBufferRedisMemory` or external storage.\\n\\nThis should give you a solid foundation for your research. Are you implementing this in a project, or comparing it to other frameworks like Haystack or LlamaIndex? I\\'d love to hear more details or help with code snippets!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1590, 'prompt_tokens': 1193, 'total_tokens': 2783, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 178, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 158}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-2301645d-2440-4208-8879-5274f16f904a-0', usage_metadata={'input_tokens': 1193, 'output_tokens': 1590, 'total_tokens': 2783, 'input_token_details': {'audio': 0, 'cache_read': 158}, 'output_token_details': {'reasoning': 178}}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, exactly! ConversationBufferMemory in LangChain is designed as a straightforward, unbounded buffer that simply accumulates and stores the *entire* conversation history as a list of messages (human inputs and AI responses). It doesn't summarize, prune, or filter anything—everything from the start of the session is kept intact and fed into the LLM's context for each new response. This makes it ideal for scenarios where you want zero loss of context, but as we discussed, it can become inefficient for longer chats due to token limits.\\n\\nIf you're testing this out or running into issues with it (like context overflow), I can suggest tweaks, such as manually clearing the buffer periodically or combining it with other memory types. What aspect are you experimenting with next?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 309, 'prompt_tokens': 2620, 'total_tokens': 2929, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 161, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1188}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-8d9d383a-9588-4d6c-bff6-76adf6416df0-0', usage_metadata={'input_tokens': 2620, 'output_tokens': 309, 'total_tokens': 2929, 'input_token_details': {'audio': 0, 'cache_read': 1188}, 'output_token_details': {'reasoning': 161}}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, that's spot on! ConversationBufferWindowMemory in LangChain maintains a sliding window of the most recent *k* exchanges (typically the last k human-AI message pairs), automatically dropping older ones to keep the buffer concise and prevent context bloat. The `k` parameter lets you control this—e.g., k=5 means it retains the last 5 back-and-forths while discarding anything before that.\\n\\nThis approach is efficient for ongoing dialogues but can lead to loss of early context, so it's best when recent history is most relevant. If you're tweaking the `k` value in code or comparing it to alternatives like summary-based memory, what's your take on its limitations so far?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 336, 'prompt_tokens': 2789, 'total_tokens': 3125, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 196, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-fbca1214-7eb1-4399-adaa-5fee2b576917-0', usage_metadata={'input_tokens': 2789, 'output_tokens': 336, 'total_tokens': 3125, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 196}})]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcR46iNxtaTS"
      },
      "source": [
        "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVtbwVytaTS",
        "outputId": "71da3ddd-990d-4e84-e437-c4665ad4b11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"James! You introduced yourself at the start of our chat. See how conversational memory keeps things connected? If there's more you'd like to dive into about memory types or implementations, I'm all ears.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 219, 'prompt_tokens': 2943, 'total_tokens': 3162, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 181, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2784}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-778aa5cc-ba40-45c7-8487-3ce04683b454-0', usage_metadata={'input_tokens': 2943, 'output_tokens': 219, 'total_tokens': 3162, 'input_token_details': {'audio': 0, 'cache_read': 2784}, 'output_token_details': {'reasoning': 181}})"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2A8VfZctaTS"
      },
      "source": [
        "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJM9TsltaTS"
      },
      "source": [
        "## 4. `ConversationSummaryBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmF85VmAtaTS"
      },
      "source": [
        "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
        "\n",
        "\n",
        "```\n",
        "# ~~ a summary of previous interactions\n",
        "The user named James introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
        "James then said he was researching the different types of conversational memory and Zeta asked for some\n",
        "examples.\n",
        "# ~~ the most recent messages\n",
        "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "Human: Buffer memory just stores the entire conversation\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tPilUOHtaTT",
        "outputId": "df54e5c6-95fc-4f98-a9ce-7f9956aab907"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-43-ed1cccbb366f>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=300,\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvUBQXmBtaTT"
      },
      "source": [
        "As before, we set up the deprecated memory type using the `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdcdtYdrtaTT"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKcrFxOtaTT"
      },
      "source": [
        "First invoke with a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAZqJKHGtaTT",
        "outputId": "d117a546-8d2a-43c0-c144-8e1629fafa89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi, my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hi, my name is James',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello, James! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hello, James! It's great to meet you! How's your day going so far?\"}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"Hi, my name is James\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-CwiwztaTT"
      },
      "source": [
        "Looks good so far, let's continue with a few more messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlsR1iaItaTT",
        "outputId": "30e93e9c-215a-48b0-f591-4d33ed0c425e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: I'm researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from past interactions for a more personalized experience. The AI outlines five types of conversational memory: short-term memory, which retains details from the current conversation; long-term memory, which remembers information across multiple interactions; contextual memory, which maintains the context of discussions; dynamic memory, which updates based on new information; and episodic memory, which recalls specific past events. The AI notes the advantages and challenges of each type, particularly concerning privacy and data management, and asks James if he is focusing on a specific type of conversational memory or exploring them all.', additional_kwargs={}, response_metadata={})]\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from past interactions for a more personalized experience. The AI outlines five types of conversational memory: short-term memory, which retains details from the current conversation; long-term memory, which remembers information across multiple interactions; contextual memory, which maintains the context of discussions; dynamic memory, which updates based on new information; and episodic memory, which recalls specific past events. The AI notes the advantages and challenges of each type, particularly concerning privacy and data management, and asks James if he is focusing on a specific type of conversational memory or exploring them all.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Oh, that's great! Both ConversationBufferMemory and ConversationBufferWindowMemory are interesting approaches to managing conversational memory. \\n\\n**ConversationBufferMemory** typically retains all the messages from the conversation, allowing the AI to reference previous exchanges in their entirety. This can be really useful for maintaining context and continuity, especially in longer conversations. However, it can also lead to challenges with memory overload if the conversation becomes too lengthy, as it may require more resources to process all that information.\\n\\nOn the other hand, **ConversationBufferWindowMemory** is a bit more streamlined. It keeps a fixed-size window of the most recent messages, which means it only remembers a certain number of exchanges at a time. This can help manage memory usage more effectively and ensure that the AI is always focused on the most relevant parts of the conversation. However, it might lose some context from earlier in the conversation if it exceeds that window size.\\n\\nAre you leaning towards one of these types for your research, or are you exploring how they compare in practical applications?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from past interactions for a more personalized experience. The AI outlines five types of conversational memory: short-term memory, long-term memory, contextual memory, dynamic memory, and episodic memory, noting their advantages and challenges, particularly regarding privacy and data management. The AI then asks James if he is focusing on a specific type of conversational memory or exploring them all. James mentions he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory retains all messages from a conversation for context but can lead to memory overload, while ConversationBufferWindowMemory keeps a fixed-size window of recent messages to manage memory usage effectively, though it may lose earlier context. The AI inquires if James is leaning towards one of these types for his research or comparing their practical applications.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's correct! Buffer memory, like ConversationBufferMemory, stores the entire conversation history, allowing the AI to access all previous messages. This can be particularly beneficial for maintaining context and ensuring that the AI can respond appropriately based on the entire dialogue. However, as I mentioned earlier, it can lead to challenges with memory overload if the conversation becomes too lengthy, as it requires more resources to manage all that information. \\n\\nIn contrast, ConversationBufferWindowMemory focuses on a fixed-size window of the most recent exchanges, which helps to keep the conversation relevant and manageable. Are you considering the implications of using one over the other in your research?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    chain.invoke({\"input\": msg})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Rj5SMftaTT"
      },
      "source": [
        "We can see with each new message the initial `SystemMessage` is updated with a new summary of the conversation. This initial `SystemMessage` is then followed by the most recent `AIMessage` and `HumanMessage` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Dnn_N8taTT"
      },
      "source": [
        "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72St8qDjtaTT"
      },
      "source": [
        "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
        "\n",
        "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWOvrR4wtaTT"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages and summarizing the messages that we\n",
        "        drop.\n",
        "        \"\"\"\n",
        "        existing_summary: SystemMessage | None = None\n",
        "        old_messages: list[BaseMessage] | None = None\n",
        "        # see if we already have a summary message\n",
        "        if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "            print(\">> Found existing summary\")\n",
        "            existing_summary = self.messages.pop(0)\n",
        "        # add the new messages to the history\n",
        "        self.messages.extend(messages)\n",
        "        # check if we have too many messages\n",
        "        if len(self.messages) > self.k:\n",
        "            print(\n",
        "                f\">> Found {len(self.messages)} messages, dropping \"\n",
        "                f\"oldest {len(self.messages) - self.k} messages.\")\n",
        "            # pull out the oldest messages...\n",
        "            old_messages = self.messages[:self.k]\n",
        "            # ...and keep only the most recent messages\n",
        "            self.messages = self.messages[-self.k:]\n",
        "        if old_messages is None:\n",
        "            print(\">> No old messages to update summary with\")\n",
        "            # if we have no old_messages, we have nothing to update in summary\n",
        "            return\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{old_messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=existing_summary,\n",
        "                old_messages=old_messages\n",
        "            )\n",
        "        )\n",
        "        print(f\">> New summary: {new_summary.content}\")\n",
        "        # prepend the new summary to the history\n",
        "        self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITlLoeAtaTT"
      },
      "source": [
        "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvWsP1GMtaTT"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HbocfKdtaTT"
      },
      "source": [
        "Setup our pipeline with new configurable fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cQ_-21_3taTU"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10GMrRXutaTU"
      },
      "source": [
        "Finally, we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfJn7DKtaTU",
        "outputId": "397a9a9c-9469-44ea-d3a5-82d8f2cab3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">> No old messages to update summary with\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 26, 'total_tokens': 37, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_bd83329f63', 'finish_reason': 'stop', 'logprobs': None}, id='run-d91fd622-92f4-45d0-8338-cd15c176e868-0', usage_metadata={'input_tokens': 26, 'output_tokens': 11, 'total_tokens': 37, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        ")\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mODJviTtaTU",
        "outputId": "fc1fb499-ab3e-426c-fe48-108da13d14e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            ">> No old messages to update summary with\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            ">> Found 6 messages, dropping latest 2 messages.\n",
            ">> New summary: James initiated the conversation by introducing himself. The AI responded warmly and inquired how it could assist him. James mentioned that he is researching different types of conversational memory. The AI provided a detailed overview of various types of conversational memory, including:\n",
            "\n",
            "1. **Short-term Memory**: Retaining information briefly for immediate responses.\n",
            "2. **Long-term Memory**: Storing information over extended periods for recalling past interactions.\n",
            "3. **Contextual Memory**: Remembering the context of conversations, including emotions and settings.\n",
            "4. **Episodic Memory**: Recalling specific events or experiences from past interactions.\n",
            "5. **Semantic Memory**: Understanding and using language and concepts relevant to discussions.\n",
            "6. **Conversational Memory in AI**: The ability of AI systems to remember past interactions to enhance user experience.\n",
            "7. **Social Memory**: The collective memory of a group that influences shared narratives.\n",
            "\n",
            "The AI encouraged James to specify any particular area of conversational memory he would like to explore further.\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping latest 2 messages.\n",
            ">> New summary: In the conversation, James introduced himself and expressed his interest in researching different types of conversational memory. The AI responded positively and provided a comprehensive overview of various types of conversational memory, including short-term, long-term, contextual, episodic, semantic, conversational memory in AI, and social memory. The AI encouraged James to specify any particular area he wanted to explore further.\n",
            "\n",
            "James then mentioned that he has been looking into **ConversationBufferMemory** and **ConversationBufferWindowMemory**. The AI explained both concepts in detail:\n",
            "\n",
            "1. **ConversationBufferMemory**: \n",
            "   - Stores the entire conversation history, allowing the AI to maintain context and continuity.\n",
            "   - Advantages include a comprehensive view of the conversation and better understanding of user intent.\n",
            "   - Disadvantages include high memory consumption and potential performance issues in long conversations.\n",
            "\n",
            "2. **ConversationBufferWindowMemory**: \n",
            "   - Maintains a fixed-size window of recent messages, making it more memory-efficient.\n",
            "   - Advantages include improved performance and reduced memory usage.\n",
            "   - Disadvantages include the risk of losing important context from earlier parts of the conversation.\n",
            "\n",
            "The AI concluded by stating that the choice between the two types of memory depends on the application's specific requirements, such as the importance of context versus memory efficiency. It invited James to ask further questions if he needed more details.\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n",
            ">> Found existing summary\n",
            ">> Found 6 messages, dropping latest 2 messages.\n",
            ">> New summary: In the conversation, James expressed his interest in researching different types of conversational memory, specifically mentioning **ConversationBufferMemory** and **ConversationBufferWindowMemory**. The AI provided a detailed overview of both concepts:\n",
            "\n",
            "1. **ConversationBufferMemory**:\n",
            "   - **Definition**: Stores the entire conversation history, allowing the AI to maintain context and continuity.\n",
            "   - **Use Cases**: Ideal for applications requiring full context, such as complex interactions or customer support.\n",
            "   - **Advantages**: Offers a comprehensive view of the conversation, enhancing understanding of user intent.\n",
            "   - **Disadvantages**: High memory consumption and potential performance issues in long conversations.\n",
            "\n",
            "2. **ConversationBufferWindowMemory**:\n",
            "   - **Definition**: Maintains a fixed-size window of recent messages, making it more memory-efficient.\n",
            "   - **Use Cases**: Suitable for short interactions where only recent context is relevant.\n",
            "   - **Advantages**: More memory-efficient and can improve performance.\n",
            "   - **Disadvantages**: Risk of losing important context from earlier parts of the conversation.\n",
            "\n",
            "The AI emphasized that the choice between the two types of memory depends on the application's specific requirements, such as the importance of context versus memory efficiency. James confirmed his understanding of buffer memory, and the AI elaborated on its characteristics, including its complete history retention, contextual understanding, and potential memory consumption issues. The AI also compared buffer memory with window memory, reiterating the trade-offs involved. The conversation concluded with an invitation for James to ask further questions if needed.\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_Xx40HtaTU"
      },
      "source": [
        "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}