{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Moeez774/Cyber-Attack-Detector/blob/master/chapters/04-chat-memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9iDOFtWugC8x"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/aurelio-labs/langchain-course/blob/main/chapters/04-chat-memory.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEZdSobItaTI"
      },
      "source": [
        "### LangChain Essentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaGqZi42taTJ"
      },
      "source": [
        "# Conversational Memory for OpenAI - LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkg11xv9taTK"
      },
      "source": [
        "Conversational memory allows our chatbots and agents to remember previous interactions within a conversation. Without conversational memory, our chatbots would only ever be able to respond to the last message they received, essentially forgetting all previous messages with each new message.\n",
        "\n",
        "Naturally, conversations require our chatbots to be able to respond over multiple interactions and refer to previous messages to understand the context of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ydYy0Wc7tfLl",
        "outputId": "758c3a0a-8d8d-4e1f-93d4-efdeb70dcf6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/412.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m409.6/412.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m333.3/333.3 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  langchain-core==0.3.33 \\\n",
        "  langchain-openai==0.3.3 \\\n",
        "  langchain-community==0.3.16 \\\n",
        "  langsmith==0.3.4"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade \"pydantic==2.10.6\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9K1xMfLPgO9f",
        "outputId": "86c612c0-08fb-424e-e998-6ddb9f12c5c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydantic==2.10.6\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6) (0.7.0)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic==2.10.6)\n",
            "  Downloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic==2.10.6) (4.15.0)\n",
            "Downloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydantic-core, pydantic\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.33.2\n",
            "    Uninstalling pydantic_core-2.33.2:\n",
            "      Successfully uninstalled pydantic_core-2.33.2\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.11.7\n",
            "    Uninstalling pydantic-2.11.7:\n",
            "      Successfully uninstalled pydantic-2.11.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mcp 1.13.1 requires pydantic<3.0.0,>=2.11.0, but you have pydantic 2.10.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pydantic-2.10.6 pydantic-core-2.27.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NIJ-etvtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ We will be using OpenAI for this example allowing us to run everything via API. If you would like to use Ollama instead, check out the [Ollama LangChain Course](https://github.com/aurelio-labs/langchain-course/tree/main/notebooks/ollama).\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euKN4BNCtaTK"
      },
      "source": [
        "---\n",
        "\n",
        "> ⚠️ If using LangSmith, add your API key below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FSfe1JIwtaTK",
        "outputId": "22deee46-58e0-4a6b-ed47-98a44fcbd034",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter LangSmith API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\") or \\\n",
        "    getpass(\"Enter LangSmith API Key: \")\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"aurelioai-langchain-course-chat-memory-openai\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZ57c5BxtaTL"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYjdtcPtaTL"
      },
      "source": [
        "## LangChain's Memory Types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czt55jO1taTL"
      },
      "source": [
        "LangChain versions `0.0.x` consisted of various conversational memory types. Most of these are due for deprecation but still hold value in understanding the different approaches that we can take to building conversational memory.\n",
        "\n",
        "Throughout the notebook we will be referring to these _older_ memory types and then rewriting them using the recommended `RunnableWithMessageHistory` class. We will learn about:\n",
        "\n",
        "* `ConversationBufferMemory`: the simplest and most intuitive form of conversational memory, keeping track of a conversation without any additional bells and whistles.\n",
        "* `ConversationBufferWindowMemory`: similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages.\n",
        "* `ConversationSummaryMemory`: rather than keeping track of the entire conversation, this memory type keeps track of a summary of the conversation.\n",
        "* `ConversationSummaryBufferMemory`: merges the `ConversationSummaryMemory` and `ConversationTokenBufferMemory` types.\n",
        "\n",
        "We'll work through each of these memory types in turn, and rewrite each one using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brmml7G-taTL"
      },
      "source": [
        "## Initialize our LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1vhQcsxtaTL"
      },
      "source": [
        "Before jumping into our memory types, let's initialize our LLM. We will use OpenAI's `gpt-4o-mini` model, if you need an API key you can get one from [OpenAI's website](https://platform.openai.com/settings/organization/api-keys)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "bNMunYQZtaTL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "OPENROUTER_API_KEY = userdata.get(\"OPENROUTER_API_KEY\")\n",
        "\n",
        "# For normal accurate responses\n",
        "llm = ChatOpenAI(temperature=0.0, api_key=OPENROUTER_API_KEY, base_url=\"https://openrouter.ai/api/v1\", model=\"x-ai/grok-4-fast:free\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wsRVD0HctaTM"
      },
      "source": [
        "## 1. `ConversationBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40zJzXXXtaTM"
      },
      "source": [
        "`ConversationBufferMemory` is the simplest form of conversational memory, it is _literally_ just a place that we store messages, and then use to feed messages into our LLM.\n",
        "\n",
        "Let's start with LangChain's original `ConversationBufferMemory` object, we are setting `return_messages=True` to return the messages as a list of `ChatMessage` objects — unless using a non-chat model we would always set this to `True` as without it the messages are passed as a direct string which can lead to unexpected behavior from chat LLMs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tY-BxLW3taTM",
        "outputId": "f0a3df15-b1dc-4ec2-ef36-eaa103193258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1448044083.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferMemory(return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory(return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMkCBXxitaTM"
      },
      "source": [
        "There are several ways that we can add messages to our memory, using the `save_context` method we can add a user query (via the `input` key) and the AI's response (via the `output` key). So, to create the following conversation:\n",
        "\n",
        "```\n",
        "User: Hi, my name is James\n",
        "AI: Hey James, what's up? I'm an AI model called Zeta.\n",
        "User: I'm researching the different types of conversational memory.\n",
        "AI: That's interesting, what are some examples?\n",
        "User: I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "User: Buffer memory just stores the entire conversation, right?\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "User: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n",
        "\n",
        "We do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mUPUE9DhtaTM"
      },
      "outputs": [],
      "source": [
        "memory.save_context(\n",
        "    {\"input\": \"Hi, my name is James\"},  # user message\n",
        "    {\"output\": \"Hey James, what's up? I'm an AI model called Zeta.\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I'm researching the different types of conversational memory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what are some examples?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"},  # user message\n",
        "    {\"output\": \"That's interesting, what's the difference?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer memory just stores the entire conversation, right?\"},  # user message\n",
        "    {\"output\": \"That makes sense, what about ConversationBufferWindowMemory?\"}  # AI response\n",
        ")\n",
        "memory.save_context(\n",
        "    {\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"},  # user message\n",
        "    {\"output\": \"Very cool!\"}  # AI response\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bw6C4W54taTM"
      },
      "source": [
        "Before using the memory, we need to load in any variables for that memory type — in this case, there are none, so we just pass an empty dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRLwtZvWtaTM",
        "outputId": "9befc2a0-eeee-4beb-89d6-47e7afe3e9bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_qjbVustaTM"
      },
      "source": [
        "With that, we've created our buffer memory. Before feeding it into our LLM let's quickly view the alternative method for adding messages to our memory. With this other method, we pass individual user and AI messages via the `add_user_message` and `add_ai_message` methods. To reproduce what we did above, we do:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOjy4g_GtaTM",
        "outputId": "62359e04-4cbe-4c13-d5c1-bf21ac622edb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "memory = ConversationBufferMemory(return_messages=True)\n",
        "\n",
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1tkc6DOtaTM"
      },
      "source": [
        "The outcome is exactly the same in either case. To pass this onto our LLM, we need to create a `ConversationChain` object — which is already deprecated in favor of the `RunnableWithMessageHistory` class, which we will cover in a moment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwgjzC0ktaTM",
        "outputId": "6becfbfb-7af0-48e3-8c7d-221f8d02acab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-839683240.py:3: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
            "  chain = ConversationChain(\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yx1fMZLWtaTN",
        "outputId": "6149ac67-27e1-467c-f7ed-26eecb5e1ad6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hey James, what's up? I'm an AI model called Zeta.\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Your name is James, right? You mentioned it at the start of our chat—nice to keep the conversation going! If I'm off base, feel free to correct me. What's next on your mind about those memory types?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Your name is James, right? You mentioned it at the start of our chat—nice to keep the conversation going! If I'm off base, feel free to correct me. What's next on your mind about those memory types?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQTzPaovtaTN"
      },
      "source": [
        "### `ConversationBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLZa82dhtaTN"
      },
      "source": [
        "As mentioned, the `ConversationBufferMemory` type is due for deprecation. Instead, we can use the `RunnableWithMessageHistory` class to implement the same functionality.\n",
        "\n",
        "When implementing `RunnableWithMessageHistory` we will use **L**ang**C**hain **E**xpression **L**anguage (LCEL) and for this we need to define our prompt template and LLM components. Our `llm` has already been defined, so now we just define a `ChatPromptTemplate` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UijZzCtutaTN"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import (\n",
        "    SystemMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        "    MessagesPlaceholder,\n",
        "    ChatPromptTemplate\n",
        ")\n",
        "\n",
        "system_prompt = \"You are a helpful assistant called Zeta.\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
        "    MessagesPlaceholder(variable_name=\"history\"),\n",
        "    HumanMessagePromptTemplate.from_template(\"{query}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnEAavfntaTN"
      },
      "source": [
        "We can link our `prompt_template` and our `llm` together to create a pipeline via LCEL."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Ywxify04taTN"
      },
      "outputs": [],
      "source": [
        "pipeline = prompt_template | llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jhOF-uVtaTN"
      },
      "source": [
        "Our `RunnableWithMessageHistory` requires our `pipeline` to be wrapped in a `RunnableWithMessageHistory` object. This object requires a few input parameters. One of those is `get_session_history`, which requires a function that returns a `ChatMessageHistory` object based on a session ID. We define this function ourselves:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BXKmcliGtaTN"
      },
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "\n",
        "chat_map = {}\n",
        "def get_chat_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = InMemoryChatMessageHistory()\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39LBschEtaTN"
      },
      "source": [
        "We also need to tell our runnable which variable name to use for the chat history (ie `history`) and which to use for the user's query (ie `query`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zHQRgNfrtaTO"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8wwEeNAVtaTO"
      },
      "source": [
        "Now we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ESDoue1taTO",
        "outputId": "8a01f283-ca2b-4e81-c8d6-3deb9de98e7b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 130, 'total_tokens': 225, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 80, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-e465056b-90a7-466e-a117-67242e557e3a-0', usage_metadata={'input_tokens': 130, 'output_tokens': 95, 'total_tokens': 225, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 80}})"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "pipeline_with_history.invoke({\n",
        "    \"query\": \"Hi my name is James\"\n",
        "}, config={ \"session_id\": \"abc_123\" })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avL-i76_taTO"
      },
      "source": [
        "Our chat history will now be memorized and retrieved whenever we invoke our runnable with the same session ID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgfzWIBstaTO",
        "outputId": "e30375c1-517d-4f01-feaa-7a857f34bc5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is James! What else can I assist you with?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 91, 'prompt_tokens': 159, 'total_tokens': 250, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 78, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-acb33858-a59c-413b-8438-6e3cb46725a6-0', usage_metadata={'input_tokens': 159, 'output_tokens': 91, 'total_tokens': 250, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 78}})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"abc_123\"}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmjSuam-taTO"
      },
      "source": [
        "We have now recreated the `ConversationBufferMemory` type using the `RunnableWithMessageHistory` class. Let's continue onto other memory types and see how these can be implemented."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0MliXn-taTO"
      },
      "source": [
        "## 2. `ConversationBufferWindowMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lziswUkqtaTO"
      },
      "source": [
        "The `ConversationBufferWindowMemory` type is similar to `ConversationBufferMemory`, but only keeps track of the last `k` messages. There are a few reasons why we would want to keep only the last `k` messages:\n",
        "\n",
        "* More messages mean more tokens are sent with each request, more tokens increases latency _and_ cost.\n",
        "\n",
        "* LLMs tend to perform worse when given more tokens, making them more likely to deviate from instructions, hallucinate, or _\"forget\"_ information provided to them. Conciseness is key to high performing LLMs.\n",
        "\n",
        "* If we keep _all_ messages we will eventually hit the LLM's context window limit, by adding a window size `k` we can ensure we never hit this limit.\n",
        "\n",
        "The buffer window solves many problems that we encounter with the standard buffer memory, while still being a very simple and intuitive form of conversational memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBb46qDDtaTO",
        "outputId": "5f7828ff-a471-4787-c828-d071ded9b2ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3216785012.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationBufferWindowMemory(k=4, return_messages=True)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4, return_messages=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to6rLd_ataTO"
      },
      "source": [
        "We populate this memory using the same methods as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pfxuLS7taTP",
        "outputId": "ed31f1ec-06dc-4e0a-f829-1e4bf12c2ff3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "memory.chat_memory.add_user_message(\"Hi, my name is James\")\n",
        "memory.chat_memory.add_ai_message(\"Hey James, what's up? I'm an AI model called Zeta.\")\n",
        "memory.chat_memory.add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what are some examples?\")\n",
        "memory.chat_memory.add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "memory.chat_memory.add_ai_message(\"That's interesting, what's the difference?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "memory.chat_memory.add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "memory.chat_memory.add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "memory.chat_memory.add_ai_message(\"Very cool!\")\n",
        "\n",
        "memory.load_memory_variables({})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV_6vOOwtaTP"
      },
      "source": [
        "As before, we use the `ConversationChain` object (again, this is deprecated and we will rewrite it with `RunnableWithMessageHistory` in a moment)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "opmjk94ltaTP"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ-g586vtaTP"
      },
      "source": [
        "Now let's see if our LLM remembers our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWa5mhzftaTP",
        "outputId": "6343bd77-d311-45f0-a761-8b412a1a329b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}), HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}), AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}), AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]\n",
            "Human: what is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'what is my name again?',\n",
              " 'history': [HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              "  HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"I'm sorry, but you haven't mentioned your name at all in our conversation so far—we've just been chatting about conversational memory types like ConversationBufferMemory and ConversationBufferWindowMemory. If you'd like to share it, I'd be happy to remember it for future reference! What's your name?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"what is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5GrFsMbStaTP"
      },
      "source": [
        "The reason our LLM can no longer remember our name is because we have set the `k` parameter to `4`, meaning that only the last messages are stored in memory, as we can see above this does not include the first message where we introduced ourselves.\n",
        "\n",
        "Based on the agent forgetting our name, we might wonder _why_ we would ever use this memory type compared to the standard buffer memory. Well, as with most things in AI, it is always a trade-off. Here we are able to support much longer conversations, use less tokens, and improve latency — but these come at the cost of forgetting non-recent messages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alyXf5nKtaTP"
      },
      "source": [
        "### `ConversationBufferWindowMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxaUDp7GtaTP"
      },
      "source": [
        "To implement this memory type using the `RunnableWithMessageHistory` class, we can use the same approach as before. We define our `prompt_template` and `llm` as before, and then wrap our pipeline in a `RunnableWithMessageHistory` object.\n",
        "\n",
        "For the window feature, we need to define a custom version of the `InMemoryChatMessageHistory` class that removes any messages beyond the last `k` messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "fQUv8yPltaTP"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain_core.chat_history import BaseChatMessageHistory\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class BufferWindowMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, k: int):\n",
        "      super().__init__(k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]):\n",
        "\n",
        "      self.messages.extend(messages)\n",
        "      self.messages = self.messages[-self.k:]\n",
        "\n",
        "    def clear(self):\n",
        "      self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "uoDJ38vztaTP"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, k: int = 4) -> BufferWindowMessageHistory:\n",
        "  if session_id not in chat_map:\n",
        "    chat_map[session_id] = BufferWindowMessageHistory(k=k)\n",
        "  return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "GsXhm811taTP"
      },
      "outputs": [],
      "source": [
        "from langchain_core.runnables import ConfigurableFieldSpec\n",
        "\n",
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQR8TZLGtaTQ"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `k` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBCnrghLtaTQ",
        "outputId": "58992f34-7c7b-4115-9fb2-7d1298db0a7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 131, 'total_tokens': 228, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-5ae54c04-1e8d-4632-b44a-0ec058b2a22c-0', usage_metadata={'input_tokens': 131, 'output_tokens': 97, 'total_tokens': 228, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 82}})"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AvqsaA3taTQ"
      },
      "source": [
        "We can also modify the messages that are stored in memory by modifying the records inside the `chat_map` dictionary directly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDMrCS26taTQ",
        "outputId": "87d83b30-58fd-45c1-afc5-7d35670a9564"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "chat_map[\"id_k4\"].clear()  # clear the history\n",
        "\n",
        "# manually insert history\n",
        "chat_map[\"id_k4\"].add_user_message(\"Hi, my name is James\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"I'm an AI model called Zeta.\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k4\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k4\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k4\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bzArAVDtaTQ"
      },
      "source": [
        "Now let's see at which `k` value our LLM remembers our name — from the above we can already see that with `k=4` our name is not mentioned, so when running with `k=4` we should expect the LLM to forget our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yn3K6fxtaTQ",
        "outputId": "2d42e2b4-9a86-408c-dce8-c21c01bf5559"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm sorry, but you haven't shared your name with me in this conversation. If you'd like to tell me, I'd be happy to remember it going forward! What's your name?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 233, 'prompt_tokens': 184, 'total_tokens': 417, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 198, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 120}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-5a8d50a5-8ad8-4717-a5d9-3381fbc8ed26-0', usage_metadata={'input_tokens': 184, 'output_tokens': 233, 'total_tokens': 417, 'input_token_details': {'audio': 0, 'cache_read': 120}, 'output_token_details': {'reasoning': 198}})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"configurable\": {\"session_id\": \"id_k4\", \"k\": 4}}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zN7am67YtaTQ"
      },
      "source": [
        "Now let's initialize a new session with `k=14`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hOXU6nI0taTQ",
        "outputId": "9f9297d5-5534-4d28-91f5-e45e8c0bdf83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 131, 'total_tokens': 239, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 93, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 130}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-2d9d276b-dd71-40a0-8be5-9c0660aa3358-0', usage_metadata={'input_tokens': 131, 'output_tokens': 108, 'total_tokens': 239, 'input_token_details': {'audio': 0, 'cache_read': 130}, 'output_token_details': {'reasoning': 93}})"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGPyLeEOtaTQ"
      },
      "source": [
        "We'll manually insert the remaining messages as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuyHMnkCtaTQ",
        "outputId": "a74cd93c-ed3d-4e36-c69f-8e18c497cf3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 131, 'total_tokens': 239, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 93, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 130}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-2d9d276b-dd71-40a0-8be5-9c0660aa3358-0', usage_metadata={'input_tokens': 131, 'output_tokens': 108, 'total_tokens': 239, 'input_token_details': {'audio': 0, 'cache_read': 130}, 'output_token_details': {'reasoning': 93}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].add_user_message(\"I'm researching the different types of conversational memory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what are some examples?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That's interesting, what's the difference?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer memory just stores the entire conversation, right?\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"That makes sense, what about ConversationBufferWindowMemory?\")\n",
        "chat_map[\"id_k14\"].add_user_message(\"Buffer window memory stores the last k messages, dropping the rest.\")\n",
        "chat_map[\"id_k14\"].add_ai_message(\"Very cool!\")\n",
        "\n",
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h8X1xV9ItaTQ",
        "outputId": "6a594c9c-4a1f-4377-9e6b-a94045214c16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your name is James! We've been chatting about conversational memory types—anything else on your mind?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 266, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 127, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a91ff8b-5aa3-43b1-a946-c492c54d7249-0', usage_metadata={'input_tokens': 266, 'output_tokens': 146, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 127}})"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"what is my name again?\"},\n",
        "    config={\"session_id\": \"id_k14\", \"k\": 14}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3SqKjEvtaTQ",
        "outputId": "b5fc74db-af19-4dc2-f7fd-178d5b701274"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 108, 'prompt_tokens': 131, 'total_tokens': 239, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 93, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 130}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-2d9d276b-dd71-40a0-8be5-9c0660aa3358-0', usage_metadata={'input_tokens': 131, 'output_tokens': 108, 'total_tokens': 239, 'input_token_details': {'audio': 0, 'cache_read': 130}, 'output_token_details': {'reasoning': 93}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what are some examples?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=\"I've been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"That's interesting, what's the difference?\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation, right?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That makes sense, what about ConversationBufferWindowMemory?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Very cool!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='what is my name again?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Your name is James! We've been chatting about conversational memory types—anything else on your mind?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 266, 'total_tokens': 412, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 127, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a91ff8b-5aa3-43b1-a946-c492c54d7249-0', usage_metadata={'input_tokens': 266, 'output_tokens': 146, 'total_tokens': 412, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 127}})]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "chat_map[\"id_k14\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R91uVCqFtaTQ"
      },
      "source": [
        "That's it! We've rewritten our buffer window memory using the recommended `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK51EzantaTR"
      },
      "source": [
        "## 3. `ConversationSummaryMemory`\n",
        "\n",
        "Next up we have `ConversationSummaryMemory`, this memory type keeps track of a summary of the conversation rather than the entire conversation. This is useful for long conversations where we don't need to keep track of the entire conversation, but we do want to keep some thread of the full conversation.\n",
        "\n",
        "As before, we'll start with the original memory class before reimplementing it with the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "m1DRv1umtaTR",
        "outputId": "b25fa40c-4052-44a4-b7bb-929891183a3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-988334424.py:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryMemory(llm=llm)\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryMemory\n",
        "\n",
        "memory = ConversationSummaryMemory(llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDK1xTiQtaTR"
      },
      "source": [
        "Unlike with the previous memory types, we need to provide an `llm` to initialize `ConversationSummaryMemory`. The reason for this is that we need an LLM to generate the conversation summaries.\n",
        "\n",
        "Beyond this small tweak, using `ConversationSummaryMemory` is the same as with our previous memory types when using the deprecated `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "A5hxfssAtaTR"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory = memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5nHb-SqtaTR"
      },
      "source": [
        "Let's test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwCtQA0RtaTR",
        "outputId": "c14c1e0a-1161-479f-8f17-50f3be41b529"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "\n",
            "Human: hello there my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI and inspired by the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, and asks what's on his mind.\n",
            "Human: I am researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI and inspired by the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, and asks what's on his mind. James says he's researching different types of conversational memory. Grok provides a detailed overview of the main types—short-term (working) memory via context windows, long-term memory using external stores like vector embeddings, implicit (parametric) memory from training data, and hybrid/multi-modal memory integrating sensory inputs—covering how they work, strengths, limitations, and examples like ChatGPT, RAG systems, and MemGPT. Grok summarizes the field's evolution and challenges (e.g., privacy, accuracy) and asks what sparked James's interest, such as building a chatbot or academic work.\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI and inspired by the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, and asks what's on his mind. James says he's researching different types of conversational memory. Grok provides a detailed overview of the main types—short-term (working) memory via context windows, long-term memory using external stores like vector embeddings, implicit (parametric) memory from training data, and hybrid/multi-modal memory integrating sensory inputs—covering how they work, strengths, limitations, and examples like ChatGPT, RAG systems, and MemGPT. Grok summarizes the field's evolution and challenges (e.g., privacy, accuracy) and asks what sparked James's interest, such as building a chatbot or academic work. James mentions he's been looking at ConversationBufferMemory and ConversationBufferWindowMemory. Grok explains these as foundational LangChain short-term memory tools for maintaining conversational context: ConversationBufferMemory appends all messages unbounded for full fidelity but risks token bloat and noise; ConversationBufferWindowMemory keeps only the recent k turns (e.g., via sliding window) for efficiency and focus but may lose early context. Grok details their mechanisms, Python code examples, strengths (simplicity for buffer, scalability for window), limitations (scalability/irrelevance for buffer, abrupt cutoffs for window), practical uses (e.g., chatbots, tutors), choice guidelines, and ties to broader LLM challenges like context limits. Grok suggests related options like summary-based memories and asks what drew James to these, if he's implementing in LangChain, or comparing to frameworks like Haystack or LlamaIndex.\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "New summary:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI and inspired by the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, and asks what's on his mind. James says he's researching different types of conversational memory. Grok provides a detailed overview of the main types—short-term (working) memory via context windows, long-term memory using external stores like vector embeddings, implicit (parametric) memory from training data, and hybrid/multi-modal memory integrating sensory inputs—covering how they work, strengths, limitations, and examples like ChatGPT, RAG systems, and MemGPT. Grok summarizes the field's evolution and challenges (e.g., privacy, accuracy) and asks what sparked James's interest, such as building a chatbot or academic work. James mentions he's been looking at ConversationBufferMemory and ConversationBufferWindowMemory. Grok explains these as foundational LangChain short-term memory tools for maintaining conversational context: ConversationBufferMemory appends all messages unbounded for full fidelity but risks token bloat and noise; ConversationBufferWindowMemory keeps only the recent k turns (e.g., via sliding window) for efficiency and focus but may lose early context. Grok details their mechanisms, Python code examples, strengths (simplicity for buffer, scalability for window), limitations (scalability/irrelevance for buffer, abrupt cutoffs for window), practical uses (e.g., chatbots, tutors), choice guidelines, and ties to broader LLM challenges like context limits. Grok suggests related options like summary-based memories and asks what drew James to these, if he's implementing in LangChain, or comparing to frameworks like Haystack or LlamaIndex. James simplifies that buffer memory just stores the entire conversation. Grok confirms this, describing it as a growing log of all messages without filtering for full context in short chats, but prone to token limits and inefficiency in longer ones; provides a Python LangChain code example for initialization and interaction, explains internal message storage, highlights strengths (fidelity for nuanced scenarios like support) and downsides (latency/costs for 20+ turns), and asks if James has encountered token overflow issues, considered switching to windowed memory, or can share his setup details.\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'Buffer window memory stores the last k messages, dropping the rest.',\n",
              " 'history': \"New summary:\\nThe human introduces himself as James. The AI introduces itself as Grok, built by xAI and inspired by the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, and asks what's on his mind. James says he's researching different types of conversational memory. Grok provides a detailed overview of the main types—short-term (working) memory via context windows, long-term memory using external stores like vector embeddings, implicit (parametric) memory from training data, and hybrid/multi-modal memory integrating sensory inputs—covering how they work, strengths, limitations, and examples like ChatGPT, RAG systems, and MemGPT. Grok summarizes the field's evolution and challenges (e.g., privacy, accuracy) and asks what sparked James's interest, such as building a chatbot or academic work. James mentions he's been looking at ConversationBufferMemory and ConversationBufferWindowMemory. Grok explains these as foundational LangChain short-term memory tools for maintaining conversational context: ConversationBufferMemory appends all messages unbounded for full fidelity but risks token bloat and noise; ConversationBufferWindowMemory keeps only the recent k turns (e.g., via sliding window) for efficiency and focus but may lose early context. Grok details their mechanisms, Python code examples, strengths (simplicity for buffer, scalability for window), limitations (scalability/irrelevance for buffer, abrupt cutoffs for window), practical uses (e.g., chatbots, tutors), choice guidelines, and ties to broader LLM challenges like context limits. Grok suggests related options like summary-based memories and asks what drew James to these, if he's implementing in LangChain, or comparing to frameworks like Haystack or LlamaIndex. James simplifies that buffer memory just stores the entire conversation. Grok confirms this, describing it as a growing log of all messages without filtering for full context in short chats, but prone to token limits and inefficiency in longer ones; provides a Python LangChain code example for initialization and interaction, explains internal message storage, highlights strengths (fidelity for nuanced scenarios like support) and downsides (latency/costs for 20+ turns), and asks if James has encountered token overflow issues, considered switching to windowed memory, or can share his setup details.\",\n",
              " 'response': 'Yes, that\\'s a spot-on simplification of how ConversationBufferWindowMemory works in LangChain! It maintains a sliding window of the most recent *k* messages (or conversation turns) from the chat history, automatically dropping older ones as the conversation grows beyond that limit. This keeps things efficient without letting the context balloon out of control, which is especially handy for LLMs with fixed token limits (like GPT-4\\'s 128k context window or even smaller models at 4k-8k tokens).\\n\\nLet me break it down a bit more for you, James, with some specifics on the mechanics, why it\\'s useful, and a quick code example to illustrate. I\\'ll draw from LangChain\\'s documentation and common implementations I\\'ve seen in practice.\\n\\n### How It Works Under the Hood\\n- **The \"Window\" Mechanism**: Unlike the plain ConversationBufferMemory, which just appends every single message (human and AI) to an ever-growing list, the Window version uses a configurable *k* parameter (default is often 10 or whatever you set). It stores messages as a list of `BaseMessage` objects (e.g., `HumanMessage` and `AIMessage` from LangChain\\'s schema). When you add a new message pair (your input + the AI\\'s response), it checks the total length. If it exceeds *k*, it prunes from the front—essentially a FIFO (first-in, first-out) queue for the oldest messages.\\n  \\n- **What Counts as a \"Message\"?**: Each turn typically includes one human message and one AI response, but it can vary. The window slides based on the total number of messages, not tokens, so you might still hit token limits if your messages are verbose. For example, if *k=5*, it keeps the last 5 messages (say, 3 human + 2 AI if the convo is uneven).\\n\\n- **Pruning Behavior**: It\\'s not fuzzy or smart about what to drop—it just chops off the earliest ones. This can lead to \"context amnesia\" if key details from the start of the conversation are vital (e.g., a user referencing an initial setup instruction 20 turns ago).\\n\\n- **Integration with Chains**: This memory type is often plugged into a LangChain `ConversationChain` or `LLMChain`. When the chain generates a response, it loads only the windowed history into the prompt template, formatted something like: \"Human: [msg1] AI: [reply1] ... Human: [latest]\".\\n\\n### Strengths\\n- **Efficiency and Scalability**: Perfect for long-running chats like customer support bots or interactive tutors, where you don\\'t need the entire history. It prevents token overflow, reduces latency (fewer tokens to process), and cuts costs on API calls (e.g., OpenAI charges per token).\\n- **Simplicity**: Easy to set up—no need for external databases or summarization logic. Great for prototypes or low-complexity apps.\\n- **Real-World Example**: In a travel booking chatbot, it might keep the last 10 exchanges focused on itinerary tweaks, dropping the initial \"Where are you flying from?\" without losing the plot.\\n\\n### Limitations\\n- **Loss of Early Context**: If the conversation loops back to old topics, the AI might seem forgetful or inconsistent. For instance, in a therapy-like bot, dropping early emotional disclosures could be problematic.\\n- **Fixed-Size Rigidity**: The *k* is static; you can\\'t dynamically adjust it mid-convo without custom code. Also, if messages vary in length, you might unpredictably lose more/less context.\\n- **No Semantic Pruning**: It doesn\\'t prioritize \"important\" messages based on content (e.g., via embeddings)—that\\'s where more advanced memories like ConversationSummaryBufferMemory come in, which summarizes the dropped parts.\\n\\n### Quick Python Code Example\\nHere\\'s a basic setup using LangChain (assuming you have `langchain` and an LLM like OpenAI installed). This initializes a chain with WindowMemory (*k=3* for brevity) and simulates a short convo:\\n\\n```python\\nfrom langchain.memory import ConversationBufferWindowMemory\\nfrom langchain.chains import ConversationChain\\nfrom langchain_openai import OpenAI  # Or use HuggingFaceHub, etc.\\n\\n# Initialize the memory with k=3 (last 3 messages)\\nmemory = ConversationBufferWindowMemory(k=3, return_messages=True)\\n\\n# Set up the LLM (replace with your API key)\\nllm = OpenAI(temperature=0.7, openai_api_key=\"your-api-key-here\")\\n\\n# Create the conversation chain\\nconversation = ConversationChain(llm=llm, memory=memory)\\n\\n# Simulate a conversation\\nprint(conversation.predict(input=\"Hi, I\\'m planning a trip to Paris.\"))\\n# Output: Something like \"Great! What dates are you thinking?\"\\n\\nprint(conversation.predict(input=\"In July, for 5 days. Suggest hotels.\"))\\n# Keeps growing...\\n\\nprint(conversation.predict(input=\"Make it budget-friendly under $200/night.\"))\\n# Now, if we check memory, it holds only the last 3 messages after a few more turns.\\n\\n# To inspect the memory:\\nprint(memory.buffer)  # Shows the list of recent messages\\n```\\n\\nAfter a few exchanges, running `print(memory.buffer)` would show only the tail end, like the last human query and AI response, dropping the Paris intro if *k* is small.\\n\\nThis pairs well with the BufferMemory you mentioned earlier—start with Buffer for short convos, switch to Window for longer ones. In fact, many devs use a hybrid: Window for immediate context + a separate summary store for long-term recall.\\n\\nHave you run into scenarios where the fixed *k* felt too blunt, James? Or are you tweaking it for a specific app, like integrating it with a vector store for RAG? I\\'d love to hear more about your setup—maybe you\\'re building something in Streamlit or FastAPI?'}"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"hello there my name is James\"})\n",
        "chain.invoke({\"input\": \"I am researching the different types of conversational memory.\"})\n",
        "chain.invoke({\"input\": \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\"})\n",
        "chain.invoke({\"input\": \"Buffer memory just stores the entire conversation\"})\n",
        "chain.invoke({\"input\": \"Buffer window memory stores the last k messages, dropping the rest.\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfgmdXd2taTR"
      },
      "source": [
        "We can see how the conversation summary varies with each new message. Let's see if the LLM is able to recall our name:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHnh4Y1OtaTR",
        "outputId": "aa19cbf3-2cba-450b-cb1f-67893b07071a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "The human introduces himself as James. The AI introduces itself as Grok, built by xAI and inspired by the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, and asks what's on his mind. James says he's researching different types of conversational memory. Grok provides a detailed overview of the main types—short-term (working) memory via context windows, long-term memory using external stores like vector embeddings, implicit (parametric) memory from training data, and hybrid/multi-modal memory integrating sensory inputs—covering how they work, strengths, limitations, and examples like ChatGPT, RAG systems, and MemGPT. Grok summarizes the field's evolution and challenges (e.g., privacy, accuracy) and asks what sparked James's interest, such as building a chatbot or academic work. James mentions he's been looking at ConversationBufferMemory and ConversationBufferWindowMemory. Grok explains these as foundational LangChain short-term memory tools for maintaining conversational context: ConversationBufferMemory appends all messages unbounded for full fidelity but risks token bloat and noise; ConversationBufferWindowMemory keeps only the recent k turns (e.g., via sliding window) for efficiency and focus but may lose early context. Grok details their mechanisms, Python code examples, strengths (simplicity for buffer, scalability for window), limitations (scalability/irrelevance for buffer, abrupt cutoffs for window), practical uses (e.g., chatbots, tutors), choice guidelines, and ties to broader LLM challenges like context limits. Grok suggests related options like summary-based memories and asks what drew James to these, if he's implementing in LangChain, or comparing to frameworks like Haystack or LlamaIndex. James simplifies that buffer memory just stores the entire conversation. Grok confirms this, describing it as a growing log of all messages without filtering for full context in short chats, but prone to token limits and inefficiency in longer ones; provides a Python LangChain code example for initialization and interaction, explains internal message storage, highlights strengths (fidelity for nuanced scenarios like support) and downsides (latency/costs for 20+ turns), and asks if James has encountered token overflow issues, considered switching to windowed memory, or can share his setup details. James further simplifies that buffer window memory stores the last k messages, dropping the rest. Grok confirms this accurate description, elaborating on its sliding window mechanics (FIFO pruning of oldest messages as a list of BaseMessage objects, with k configurable, e.g., default 10), what counts as a message, integration with LangChain chains, strengths (efficiency, scalability, cost savings for long chats like support bots), limitations (loss of early context, fixed rigidity, no semantic pruning), and a Python code example for setup with OpenAI in a ConversationChain (k=3 for demo, showing buffer inspection). Grok notes hybrids with summaries for better recall and asks if James has faced issues with fixed k, is tweaking for a specific app, or integrating with tools like vector stores, Streamlit, or FastAPI.\n",
            "Human: What is my name again?\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input': 'What is my name again?',\n",
              " 'history': \"The human introduces himself as James. The AI introduces itself as Grok, built by xAI and inspired by the Hitchhiker's Guide to the Galaxy and JARVIS from Iron Man, and asks what's on his mind. James says he's researching different types of conversational memory. Grok provides a detailed overview of the main types—short-term (working) memory via context windows, long-term memory using external stores like vector embeddings, implicit (parametric) memory from training data, and hybrid/multi-modal memory integrating sensory inputs—covering how they work, strengths, limitations, and examples like ChatGPT, RAG systems, and MemGPT. Grok summarizes the field's evolution and challenges (e.g., privacy, accuracy) and asks what sparked James's interest, such as building a chatbot or academic work. James mentions he's been looking at ConversationBufferMemory and ConversationBufferWindowMemory. Grok explains these as foundational LangChain short-term memory tools for maintaining conversational context: ConversationBufferMemory appends all messages unbounded for full fidelity but risks token bloat and noise; ConversationBufferWindowMemory keeps only the recent k turns (e.g., via sliding window) for efficiency and focus but may lose early context. Grok details their mechanisms, Python code examples, strengths (simplicity for buffer, scalability for window), limitations (scalability/irrelevance for buffer, abrupt cutoffs for window), practical uses (e.g., chatbots, tutors), choice guidelines, and ties to broader LLM challenges like context limits. Grok suggests related options like summary-based memories and asks what drew James to these, if he's implementing in LangChain, or comparing to frameworks like Haystack or LlamaIndex. James simplifies that buffer memory just stores the entire conversation. Grok confirms this, describing it as a growing log of all messages without filtering for full context in short chats, but prone to token limits and inefficiency in longer ones; provides a Python LangChain code example for initialization and interaction, explains internal message storage, highlights strengths (fidelity for nuanced scenarios like support) and downsides (latency/costs for 20+ turns), and asks if James has encountered token overflow issues, considered switching to windowed memory, or can share his setup details. James further simplifies that buffer window memory stores the last k messages, dropping the rest. Grok confirms this accurate description, elaborating on its sliding window mechanics (FIFO pruning of oldest messages as a list of BaseMessage objects, with k configurable, e.g., default 10), what counts as a message, integration with LangChain chains, strengths (efficiency, scalability, cost savings for long chats like support bots), limitations (loss of early context, fixed rigidity, no semantic pruning), and a Python code example for setup with OpenAI in a ConversationChain (k=3 for demo, showing buffer inspection). Grok notes hybrids with summaries for better recall and asks if James has faced issues with fixed k, is tweaking for a specific app, or integrating with tools like vector stores, Streamlit, or FastAPI.\",\n",
              " 'response': \"Your name is James! We've been chatting about conversational memory types in AI systems, like those LangChain tools you mentioned—ConversationBufferMemory and ConversationBufferWindowMemory. It's a fascinating topic, especially since it ties right into how models like me maintain context across turns (spoiler: I use a combination of short-term context windows and broader parametric knowledge from training). Did that jog your memory, or is there something specific about our discussion you'd like to dive deeper into?\"}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"What is my name again?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZpfPyEztaTR"
      },
      "source": [
        "As this information was stored in the summary the LLM successfully recalled our name. This may not always be the case, by summarizing the conversation we inevitably compress the full amount of information and so we may lose key details occasionally. Nonetheless, this is a great memory type for long conversations while retaining some key information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snlKvJDUtaTR"
      },
      "source": [
        "### `ConversationSummaryMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9jdJCkztaTR"
      },
      "source": [
        "Let's implement this memory type using the `RunnableWithMessageHistory` class. As with the window buffer memory, we need to define a custom implementation of the `InMemoryChatMessageHistory` class. We'll call this one `ConversationSummaryMessageHistory`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "C09U1WkqtaTR"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "\n",
        "class ConversationSummaryMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI):\n",
        "        super().__init__(llm=llm)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "        \"\"\"Add messages to the history, removing any messages beyond\n",
        "        the last `k` messages.\n",
        "        \"\"\"\n",
        "        self.messages.extend(messages)\n",
        "        # construct the summary chat messages\n",
        "        summary_prompt = ChatPromptTemplate.from_messages([\n",
        "            SystemMessagePromptTemplate.from_template(\n",
        "                \"Given the existing conversation summary and the new messages, \"\n",
        "                \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                \"as much relevant information as possible.\"\n",
        "            ),\n",
        "            HumanMessagePromptTemplate.from_template(\n",
        "                \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                \"New messages:\\n{messages}\"\n",
        "            )\n",
        "        ])\n",
        "        # format the messages and invoke the LLM\n",
        "        new_summary = self.llm.invoke(\n",
        "            summary_prompt.format_messages(\n",
        "                existing_summary=self.messages.content,\n",
        "                messages=[x.content for x in messages]\n",
        "            )\n",
        "        )\n",
        "        # replace the existing history with a single system summary message\n",
        "        self.messages = [SystemMessage(content=new_summary.content)]\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "762Vg5u4taTR"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI) -> ConversationSummaryMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryMessageHistory(llm=llm)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "om78sKCGtaTS"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPfut3S5taTS"
      },
      "source": [
        "Now we invoke our runnable, this time passing a `llm` parameter via the `config` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1F1m9n2taTS",
        "outputId": "c996c578-3822-4d53-ad10-108a9167c74c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi James! Nice to meet you. I'm Zeta, your helpful assistant. What can I do for you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 75, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 130}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-d8956b72-df0b-4608-b96f-7764cc38c18a-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 130}, 'output_token_details': {'reasoning': 75}})"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYqOCHCTtaTS"
      },
      "source": [
        "Let's see what summary was generated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBDeudJftaTS",
        "outputId": "0d637ca5-1a80-4ed1-f4b0-072aab81dd41"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Hi James! Nice to meet you. I'm Zeta, your helpful assistant. What can I do for you today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 75, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 130}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-d8956b72-df0b-4608-b96f-7764cc38c18a-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 130}, 'output_token_details': {'reasoning': 75}})]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6uLeGXztaTS"
      },
      "source": [
        "Let's continue the conversation and see if the summary is updated:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Di7WVWmCtaTS",
        "outputId": "f18c0475-0b35-4d90-d405-74afb5924a87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a5ffaf0-771f-4a62-9549-cc9682d94f81-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 83}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s a fascinating topic! Conversational memory refers to how systems (like AI chatbots or humans) retain, recall, and use information from past interactions to make dialogues more coherent and context-aware. It draws from fields like AI, cognitive psychology, and linguistics. I\\'ll break down the main types of conversational memory below, focusing on both human and AI perspectives for a well-rounded view. If you\\'re leaning toward a specific angle (e.g., AI implementation or psychological models), let me know!\\n\\n### 1. **Short-Term (or Working) Memory**\\n   - **Description**: This is the temporary storage of information relevant to the immediate conversation. It holds details from the recent exchange but fades quickly without reinforcement.\\n   - **In Humans**: Based on models like Baddeley\\'s working memory framework, it includes a \"phonological loop\" for verbal info and a \"visuospatial sketchpad\" for non-verbal cues. Capacity is limited (about 7±2 items, per Miller\\'s Law).\\n   - **In AI**: Often tied to the model\\'s context window (e.g., in GPT models, this can be thousands of tokens). It keeps track of the ongoing chat but resets between sessions unless extended.\\n   - **Example**: In a chat, remembering the last few sentences to respond appropriately, like recalling you said \"Hi, my name is James\" just moments ago.\\n   - **Strengths/Limitations**: Fast and efficient but prone to overload or forgetting if the conversation gets too long.\\n\\n### 2. **Long-Term Memory**\\n   - **Description**: Persistent storage of information from past conversations, allowing recall over time, sessions, or even indefinitely.\\n   - **Subtypes**:\\n     - **Episodic Memory**: Stores specific events or interactions with context (who, what, when). It\\'s autobiographical and time-stamped.\\n       - Human Example: Recalling a fun debate you had with a friend last week.\\n       - AI Example: A chatbot like me remembering your previous query about conversational memory in a future session (if the system supports it).\\n     - **Semantic Memory**: Stores general facts, patterns, or knowledge extracted from conversations, without tying to specific events.\\n       - Human Example: Knowing that \"conversational memory\" relates to AI after reading about it multiple times.\\n       - AI Example: Learning user preferences, like your interest in research topics, to tailor responses.\\n     - **Procedural Memory**: Implicit knowledge of how to conduct conversations (e.g., turn-taking, politeness norms). This is more \"how-to\" than \"what.\"\\n   - **In AI**: Implemented via databases, vector embeddings (e.g., in retrieval-augmented generation or RAG systems), or fine-tuning. Tools like LangChain or Pinecone help manage this.\\n   - **Strengths/Limitations**: Builds continuity and personalization but can lead to privacy issues or outdated info if not updated.\\n\\n### 3. **Contextual or Session-Based Memory**\\n   - **Description**: A hybrid that maintains state within a single conversation or session but doesn\\'t persist long-term. It\\'s about threading the dialogue logically.\\n   - **In Humans**: Relies on cues like shared references (e.g., \"Remember that story I told you?\") to jog recall.\\n   - **In AI**: Uses conversation history buffers or state machines. For instance, in multi-turn dialogues, the model references prior messages to avoid repetition.\\n   - **Example**: In this chat, I\\'m using contextual memory to know you\\'re researching this topic and build on your intro.\\n   - **Strengths/Limitations**: Enhances flow but can break if the session ends abruptly.\\n\\n### 4. **External or Augmented Memory**\\n   - **Description**: Offloads memory to external tools, like notes, databases, or search engines, to extend capacity beyond internal limits.\\n   - **In Humans**: Using a notebook or phone reminders during talks.\\n   - **In AI**: Integrates with APIs (e.g., searching the web for facts) or memory stores like Redis. Emerging tech like \"memory graphs\" in AI agents (e.g., in AutoGPT) links related concepts dynamically.\\n   - **Example**: An AI pulling up a user\\'s past emails to reference in a support chat.\\n   - **Strengths/Limitations**: Scalable and accurate but depends on reliable external systems.\\n\\n### Key Challenges and Trends\\n- **Challenges**: Forgetting (catastrophic forgetting in AI training), privacy (storing personal data), and bias (recalling skewed info).\\n- **Trends**: In AI, we\\'re seeing advances like infinite context windows (e.g., via sparse attention) and hybrid human-AI memory systems. Research in NLP (natural language processing) often cites models like those from OpenAI or Google for inspiration.\\n\\nIf you\\'d like resources (books like \"The Psychology of Conversation\" or papers on arXiv), examples from real AI systems, or how this applies to a specific use case, just say the word. What\\'s sparking your interest in this?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1184, 'prompt_tokens': 163, 'total_tokens': 1347, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 176, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 138}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-873c13e6-6a93-472b-9cdf-fe2027b404e4-0', usage_metadata={'input_tokens': 163, 'output_tokens': 1184, 'total_tokens': 1347, 'input_token_details': {'audio': 0, 'cache_read': 138}, 'output_token_details': {'reasoning': 176}})]"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"I'm researching the different types of conversational memory.\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")\n",
        "\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqEv8kxZtaTS"
      },
      "source": [
        "So far so good! Let's continue with a few more messages before returning to the name question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7cy8dKZtaTS",
        "outputId": "a5ce2759-4006-453d-b170-2438c186fd3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n",
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnOdFy4htaTS"
      },
      "source": [
        "Let's see the latest summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SEBczVutaTS",
        "outputId": "3104b9a0-5ddb-4be3-c0a5-01acc9c1fe6f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 98, 'prompt_tokens': 131, 'total_tokens': 229, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 83, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-7a5ffaf0-771f-4a62-9549-cc9682d94f81-0', usage_metadata={'input_tokens': 131, 'output_tokens': 98, 'total_tokens': 229, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 83}}),\n",
              " HumanMessage(content=\"I'm researching the different types of conversational memory.\", additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s a fascinating topic! Conversational memory refers to how systems (like AI chatbots or humans) retain, recall, and use information from past interactions to make dialogues more coherent and context-aware. It draws from fields like AI, cognitive psychology, and linguistics. I\\'ll break down the main types of conversational memory below, focusing on both human and AI perspectives for a well-rounded view. If you\\'re leaning toward a specific angle (e.g., AI implementation or psychological models), let me know!\\n\\n### 1. **Short-Term (or Working) Memory**\\n   - **Description**: This is the temporary storage of information relevant to the immediate conversation. It holds details from the recent exchange but fades quickly without reinforcement.\\n   - **In Humans**: Based on models like Baddeley\\'s working memory framework, it includes a \"phonological loop\" for verbal info and a \"visuospatial sketchpad\" for non-verbal cues. Capacity is limited (about 7±2 items, per Miller\\'s Law).\\n   - **In AI**: Often tied to the model\\'s context window (e.g., in GPT models, this can be thousands of tokens). It keeps track of the ongoing chat but resets between sessions unless extended.\\n   - **Example**: In a chat, remembering the last few sentences to respond appropriately, like recalling you said \"Hi, my name is James\" just moments ago.\\n   - **Strengths/Limitations**: Fast and efficient but prone to overload or forgetting if the conversation gets too long.\\n\\n### 2. **Long-Term Memory**\\n   - **Description**: Persistent storage of information from past conversations, allowing recall over time, sessions, or even indefinitely.\\n   - **Subtypes**:\\n     - **Episodic Memory**: Stores specific events or interactions with context (who, what, when). It\\'s autobiographical and time-stamped.\\n       - Human Example: Recalling a fun debate you had with a friend last week.\\n       - AI Example: A chatbot like me remembering your previous query about conversational memory in a future session (if the system supports it).\\n     - **Semantic Memory**: Stores general facts, patterns, or knowledge extracted from conversations, without tying to specific events.\\n       - Human Example: Knowing that \"conversational memory\" relates to AI after reading about it multiple times.\\n       - AI Example: Learning user preferences, like your interest in research topics, to tailor responses.\\n     - **Procedural Memory**: Implicit knowledge of how to conduct conversations (e.g., turn-taking, politeness norms). This is more \"how-to\" than \"what.\"\\n   - **In AI**: Implemented via databases, vector embeddings (e.g., in retrieval-augmented generation or RAG systems), or fine-tuning. Tools like LangChain or Pinecone help manage this.\\n   - **Strengths/Limitations**: Builds continuity and personalization but can lead to privacy issues or outdated info if not updated.\\n\\n### 3. **Contextual or Session-Based Memory**\\n   - **Description**: A hybrid that maintains state within a single conversation or session but doesn\\'t persist long-term. It\\'s about threading the dialogue logically.\\n   - **In Humans**: Relies on cues like shared references (e.g., \"Remember that story I told you?\") to jog recall.\\n   - **In AI**: Uses conversation history buffers or state machines. For instance, in multi-turn dialogues, the model references prior messages to avoid repetition.\\n   - **Example**: In this chat, I\\'m using contextual memory to know you\\'re researching this topic and build on your intro.\\n   - **Strengths/Limitations**: Enhances flow but can break if the session ends abruptly.\\n\\n### 4. **External or Augmented Memory**\\n   - **Description**: Offloads memory to external tools, like notes, databases, or search engines, to extend capacity beyond internal limits.\\n   - **In Humans**: Using a notebook or phone reminders during talks.\\n   - **In AI**: Integrates with APIs (e.g., searching the web for facts) or memory stores like Redis. Emerging tech like \"memory graphs\" in AI agents (e.g., in AutoGPT) links related concepts dynamically.\\n   - **Example**: An AI pulling up a user\\'s past emails to reference in a support chat.\\n   - **Strengths/Limitations**: Scalable and accurate but depends on reliable external systems.\\n\\n### Key Challenges and Trends\\n- **Challenges**: Forgetting (catastrophic forgetting in AI training), privacy (storing personal data), and bias (recalling skewed info).\\n- **Trends**: In AI, we\\'re seeing advances like infinite context windows (e.g., via sparse attention) and hybrid human-AI memory systems. Research in NLP (natural language processing) often cites models like those from OpenAI or Google for inspiration.\\n\\nIf you\\'d like resources (books like \"The Psychology of Conversation\" or papers on arXiv), examples from real AI systems, or how this applies to a specific use case, just say the word. What\\'s sparking your interest in this?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1184, 'prompt_tokens': 163, 'total_tokens': 1347, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 176, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 138}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-873c13e6-6a93-472b-9cdf-fe2027b404e4-0', usage_metadata={'input_tokens': 163, 'output_tokens': 1184, 'total_tokens': 1347, 'input_token_details': {'audio': 0, 'cache_read': 138}, 'output_token_details': {'reasoning': 176}}),\n",
              " HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That\\'s great—diving into LangChain\\'s memory implementations is a smart way to explore practical conversational memory in AI systems. ConversationBufferMemory and ConversationBufferWindowMemory are two foundational classes in LangChain (a Python framework for LLM chains and agents) designed to handle chat history for more context-aware interactions. They both store conversation data as a simple buffer of messages but differ in how they manage size and retention. I\\'ll explain each, compare them, and touch on implementation basics. If you\\'re coding this or need examples, let me know!\\n\\n### 1. **ConversationBufferMemory**\\n   - **Description**: This is the simplest form of memory in LangChain. It acts like an unbounded buffer that stores *every* message in the conversation history, from the very first exchange onward. Each new human or AI message is appended to the buffer, and the full history is passed to the LLM (large language model) with every subsequent call.\\n   - **How It Works**:\\n     - Messages are stored as a list of `HumanMessage` and `AIMessage` objects (or similar, depending on the chat model).\\n     - When generating a response, the entire buffer is injected into the prompt as context (e.g., via a template like \"Human: {input}\\\\nAI: {history}\\\\nHuman:\").\\n     - No automatic pruning—it\\'s \"infinite\" until you manually clear or limit it.\\n   - **Use Cases**:\\n     - Short, focused conversations where full history is valuable and token limits aren\\'t a concern (e.g., a quick Q&A bot).\\n     - Prototyping or debugging, as it preserves everything for easy inspection.\\n   - **Pros**:\\n     - Simple to implement and understand.\\n     - Maintains complete context, reducing \"forgetfulness\" in ongoing dialogues.\\n   - **Cons**:\\n     - Can quickly exceed LLM context windows (e.g., 4K–128K tokens in models like GPT-3.5 or GPT-4), leading to high costs, slower responses, or truncation.\\n     - Not scalable for long-term or high-volume chats—performance degrades as history grows.\\n   - **Basic Implementation Example (Python with LangChain)**:\\n     ```python\\n     from langchain.memory import ConversationBufferMemory\\n     from langchain.chains import ConversationChain\\n     from langchain.llms import OpenAI  # Or your preferred LLM\\n\\n     llm = OpenAI(temperature=0)\\n     memory = ConversationBufferMemory()\\n     conversation = ConversationChain(llm=llm, memory=memory)\\n\\n     # First interaction\\n     response1 = conversation.predict(input=\"Hi, I\\'m researching memory.\")\\n     print(response1)  # AI responds\\n\\n     # Second interaction—full history is used\\n     response2 = conversation.predict(input=\"Tell me about buffers.\")\\n     print(response2)\\n     print(memory.buffer)  # Shows full chat history\\n     ```\\n     Here, `memory.buffer` holds the entire transcript.\\n\\n### 2. **ConversationBufferWindowMemory**\\n   - **Description**: This builds on the buffer concept but adds a \"sliding window\" mechanism to limit memory to the most recent *k* interactions (where k is a configurable parameter, defaulting to 2). Older messages are automatically dropped as the conversation progresses, keeping the buffer size manageable.\\n   - **How It Works**:\\n     - Like the buffer memory, it stores messages in a list, but only the last k exchanges (human + AI pairs) are retained.\\n     - For example, if k=5, it keeps the last 5 human messages and their corresponding AI responses.\\n     - The window \"slides\" forward with each new message, pruning from the beginning.\\n     - You can also specify a `k` value and optionally return only messages (not full history) for efficiency.\\n   - **Use Cases**:\\n     - Medium-length conversations where recent context matters most (e.g., customer support bots handling sequential queries without needing ancient history).\\n     - Resource-constrained environments, like mobile apps or edge devices, to avoid token bloat.\\n   - **Pros**:\\n     - Balances context retention with efficiency—prevents context windows from overflowing.\\n     - More performant and cost-effective for longer sessions.\\n     - Easy to tune via the `k` parameter (e.g., set to 10 for more recall).\\n   - **Cons**:\\n     - Loses older context, which can cause \"amnesia\" if a user references early details (e.g., \"Remember what I said about James?\").\\n     - Requires careful choice of k; too small, and responses feel disjointed; too large, and it mimics the full buffer.\\n   - **Basic Implementation Example (Python with LangChain)**:\\n     ```python\\n     from langchain.memory import ConversationBufferWindowMemory\\n     from langchain.chains import ConversationChain\\n     from langchain.llms import OpenAI\\n\\n     llm = OpenAI(temperature=0)\\n     memory = ConversationBufferWindowMemory(k=2)  # Keep last 2 exchanges\\n     conversation = ConversationChain(llm=llm, memory=memory)\\n\\n     # Interactions—after 3+ exchanges, oldest are dropped\\n     response1 = conversation.predict(input=\"Hi, I\\'m James.\")\\n     response2 = conversation.predict(input=\"What\\'s short-term memory?\")\\n     response3 = conversation.predict(input=\"Now explain long-term.\")  # Only last 2 used\\n     print(response3)\\n     print(memory.buffer)  # Shows only recent history\\n     ```\\n\\n### Comparison and When to Choose\\n- **Similarities**: Both are lightweight, human-readable (store plain message lists), and integrate seamlessly with LangChain chains/agents. They\\'re great for basic stateful conversations without needing complex databases.\\n- **Key Differences**:\\n  | Aspect              | ConversationBufferMemory | ConversationBufferWindowMemory |\\n  |---------------------|---------------------------|--------------------------------|\\n  | **History Retention** | Full (unlimited)         | Limited to last k exchanges   |\\n  | **Scalability**     | Poor for long chats      | Better; controls token usage  |\\n  | **Complexity**      | Very simple              | Slightly more (tune k)        |\\n  | **Best For**        | Short, complete recall   | Ongoing, efficient dialogues  |\\n- **Trade-offs**: If your app needs persistent, long-term recall (e.g., across sessions), consider upgrading to something like `ConversationSummaryMemory` (which summarizes history) or `ConversationEntityMemory` (extracts key entities). For production, pair these with vector stores for semantic search.\\n- **Limitations in Practice**: These are in-memory only, so they don\\'t persist across restarts. For durability, use `ConversationBufferRedisMemory` or external storage.\\n\\nThis should give you a solid foundation for your research. Are you implementing this in a project, or comparing it to other frameworks like Haystack or LlamaIndex? I\\'d love to hear more details or help with code snippets!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 1590, 'prompt_tokens': 1193, 'total_tokens': 2783, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 178, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 158}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-2301645d-2440-4208-8879-5274f16f904a-0', usage_metadata={'input_tokens': 1193, 'output_tokens': 1590, 'total_tokens': 2783, 'input_token_details': {'audio': 0, 'cache_read': 158}, 'output_token_details': {'reasoning': 178}}),\n",
              " HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, exactly! ConversationBufferMemory in LangChain is designed as a straightforward, unbounded buffer that simply accumulates and stores the *entire* conversation history as a list of messages (human inputs and AI responses). It doesn't summarize, prune, or filter anything—everything from the start of the session is kept intact and fed into the LLM's context for each new response. This makes it ideal for scenarios where you want zero loss of context, but as we discussed, it can become inefficient for longer chats due to token limits.\\n\\nIf you're testing this out or running into issues with it (like context overflow), I can suggest tweaks, such as manually clearing the buffer periodically or combining it with other memory types. What aspect are you experimenting with next?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 309, 'prompt_tokens': 2620, 'total_tokens': 2929, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 161, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1188}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-8d9d383a-9588-4d6c-bff6-76adf6416df0-0', usage_metadata={'input_tokens': 2620, 'output_tokens': 309, 'total_tokens': 2929, 'input_token_details': {'audio': 0, 'cache_read': 1188}, 'output_token_details': {'reasoning': 161}}),\n",
              " HumanMessage(content='Buffer window memory stores the last k messages, dropping the rest.', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Yes, that's spot on! ConversationBufferWindowMemory in LangChain maintains a sliding window of the most recent *k* exchanges (typically the last k human-AI message pairs), automatically dropping older ones to keep the buffer concise and prevent context bloat. The `k` parameter lets you control this—e.g., k=5 means it retains the last 5 back-and-forths while discarding anything before that.\\n\\nThis approach is efficient for ongoing dialogues but can lead to loss of early context, so it's best when recent history is most relevant. If you're tweaking the `k` value in code or comparing it to alternatives like summary-based memory, what's your take on its limitations so far?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 336, 'prompt_tokens': 2789, 'total_tokens': 3125, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 196, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 112}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-fbca1214-7eb1-4399-adaa-5fee2b576917-0', usage_metadata={'input_tokens': 2789, 'output_tokens': 336, 'total_tokens': 3125, 'input_token_details': {'audio': 0, 'cache_read': 112}, 'output_token_details': {'reasoning': 196}})]"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ],
      "source": [
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcR46iNxtaTS"
      },
      "source": [
        "The information about our name has been maintained, so let's see if this is enough for our LLM to correctly recall our name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doVtbwVytaTS",
        "outputId": "71da3ddd-990d-4e84-e437-c4665ad4b11c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in RootListenersTracer.on_chain_end callback: AttributeError(\"'list' object has no attribute 'content'\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"James! You introduced yourself at the start of our chat. See how conversational memory keeps things connected? If there's more you'd like to dive into about memory types or implementations, I'm all ears.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 219, 'prompt_tokens': 2943, 'total_tokens': 3162, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 181, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 2784}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-778aa5cc-ba40-45c7-8487-3ce04683b454-0', usage_metadata={'input_tokens': 2943, 'output_tokens': 219, 'total_tokens': 3162, 'input_token_details': {'audio': 0, 'cache_read': 2784}, 'output_token_details': {'reasoning': 181}})"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"What is my name again?\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm}\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2A8VfZctaTS"
      },
      "source": [
        "Perfect! We've successfully implemented the `ConversationSummaryMemory` type using the `RunnableWithMessageHistory` class."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpJM9TsltaTS"
      },
      "source": [
        "## 4. `ConversationSummaryBufferMemory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmF85VmAtaTS"
      },
      "source": [
        "Our final memory type acts as a combination of `ConversationSummaryMemory` and `ConversationBufferMemory`. It keeps the buffer for the conversation up until the previous `n` tokens, anything beyond that limit is summarized then dropped from the buffer. Producing something like:\n",
        "\n",
        "\n",
        "```\n",
        "# ~~ a summary of previous interactions\n",
        "The user named James introduced himself and the AI responded, introducing itself as an AI model called Zeta.\n",
        "James then said he was researching the different types of conversational memory and Zeta asked for some\n",
        "examples.\n",
        "# ~~ the most recent messages\n",
        "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
        "AI: That's interesting, what's the difference?\n",
        "Human: Buffer memory just stores the entire conversation\n",
        "AI: That makes sense, what about ConversationBufferWindowMemory?\n",
        "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
        "AI: Very cool!\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tPilUOHtaTT",
        "outputId": "df54e5c6-95fc-4f98-a9ce-7f9956aab907"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-43-ed1cccbb366f>:3: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
            "  memory = ConversationSummaryBufferMemory(\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationSummaryBufferMemory\n",
        "\n",
        "memory = ConversationSummaryBufferMemory(\n",
        "    llm=llm,\n",
        "    max_token_limit=300,\n",
        "    return_messages=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvUBQXmBtaTT"
      },
      "source": [
        "As before, we set up the deprecated memory type using the `ConversationChain` object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdcdtYdrtaTT"
      },
      "outputs": [],
      "source": [
        "chain = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsKcrFxOtaTT"
      },
      "source": [
        "First invoke with a single message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MAZqJKHGtaTT",
        "outputId": "d117a546-8d2a-43c0-c144-8e1629fafa89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[]\n",
            "Human: Hi, my name is James\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hi, my name is James',\n",
              " 'history': [HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              "  AIMessage(content=\"Hello, James! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})],\n",
              " 'response': \"Hello, James! It's great to meet you! How's your day going so far?\"}"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({\"input\": \"Hi, my name is James\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma-CwiwztaTT"
      },
      "source": [
        "Looks good so far, let's continue with a few more messages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tlsR1iaItaTT",
        "outputId": "30e93e9c-215a-48b0-f591-4d33ed0c425e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hello, James! It's great to meet you! How's your day going so far?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: I'm researching the different types of conversational memory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from past interactions for a more personalized experience. The AI outlines five types of conversational memory: short-term memory, which retains details from the current conversation; long-term memory, which remembers information across multiple interactions; contextual memory, which maintains the context of discussions; dynamic memory, which updates based on new information; and episodic memory, which recalls specific past events. The AI notes the advantages and challenges of each type, particularly concerning privacy and data management, and asks James if he is focusing on a specific type of conversational memory or exploring them all.', additional_kwargs={}, response_metadata={})]\n",
            "Human: I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from past interactions for a more personalized experience. The AI outlines five types of conversational memory: short-term memory, which retains details from the current conversation; long-term memory, which remembers information across multiple interactions; contextual memory, which maintains the context of discussions; dynamic memory, which updates based on new information; and episodic memory, which recalls specific past events. The AI notes the advantages and challenges of each type, particularly concerning privacy and data management, and asks James if he is focusing on a specific type of conversational memory or exploring them all.', additional_kwargs={}, response_metadata={}), HumanMessage(content='I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Oh, that's great! Both ConversationBufferMemory and ConversationBufferWindowMemory are interesting approaches to managing conversational memory. \\n\\n**ConversationBufferMemory** typically retains all the messages from the conversation, allowing the AI to reference previous exchanges in their entirety. This can be really useful for maintaining context and continuity, especially in longer conversations. However, it can also lead to challenges with memory overload if the conversation becomes too lengthy, as it may require more resources to process all that information.\\n\\nOn the other hand, **ConversationBufferWindowMemory** is a bit more streamlined. It keeps a fixed-size window of the most recent messages, which means it only remembers a certain number of exchanges at a time. This can help manage memory usage more effectively and ensure that the AI is always focused on the most relevant parts of the conversation. However, it might lose some context from earlier in the conversation if it exceeds that window size.\\n\\nAre you leaning towards one of these types for your research, or are you exploring how they compare in practical applications?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer memory just stores the entire conversation\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "[SystemMessage(content='The human, named James, introduces himself and mentions he is researching different types of conversational memory. The AI responds enthusiastically, explaining that conversational memory allows systems to remember information from past interactions for a more personalized experience. The AI outlines five types of conversational memory: short-term memory, long-term memory, contextual memory, dynamic memory, and episodic memory, noting their advantages and challenges, particularly regarding privacy and data management. The AI then asks James if he is focusing on a specific type of conversational memory or exploring them all. James mentions he has been looking at ConversationBufferMemory and ConversationBufferWindowMemory. The AI explains that ConversationBufferMemory retains all messages from a conversation for context but can lead to memory overload, while ConversationBufferWindowMemory keeps a fixed-size window of recent messages to manage memory usage effectively, though it may lose earlier context. The AI inquires if James is leaning towards one of these types for his research or comparing their practical applications.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Buffer memory just stores the entire conversation', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's correct! Buffer memory, like ConversationBufferMemory, stores the entire conversation history, allowing the AI to access all previous messages. This can be particularly beneficial for maintaining context and ensuring that the AI can respond appropriately based on the entire dialogue. However, as I mentioned earlier, it can lead to challenges with memory overload if the conversation becomes too lengthy, as it requires more resources to manage all that information. \\n\\nIn contrast, ConversationBufferWindowMemory focuses on a fixed-size window of the most recent exchanges, which helps to keep the conversation relevant and manageable. Are you considering the implications of using one over the other in your research?\", additional_kwargs={}, response_metadata={})]\n",
            "Human: Buffer window memory stores the last k messages, dropping the rest.\n",
            "AI:\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "for msg in [\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]:\n",
        "    chain.invoke({\"input\": msg})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Rj5SMftaTT"
      },
      "source": [
        "We can see with each new message the initial `SystemMessage` is updated with a new summary of the conversation. This initial `SystemMessage` is then followed by the most recent `AIMessage` and `HumanMessage` objects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "24Dnn_N8taTT"
      },
      "source": [
        "### `ConversationSummaryBufferMemory` with `RunnableWithMessageHistory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72St8qDjtaTT"
      },
      "source": [
        "As with the previous memory types, we will implement this memory type again using the `RunnableWithMessageHistory` class. In our implementation we will modify the buffer window to be based on the number of messages rather than number of tokens. This tweak will make our implementation more closely aligned with original buffer window.\n",
        "\n",
        "We will implement all of this via a new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "tWOvrR4wtaTT"
      },
      "outputs": [],
      "source": [
        "class ConversationSummaryBufferMessageHistory(BaseChatMessageHistory, BaseModel):\n",
        "    messages: list[BaseMessage] = Field(default_factory=list)\n",
        "    llm: ChatOpenAI = Field(default_factory=ChatOpenAI)\n",
        "    k: int = Field(default_factory=int)\n",
        "\n",
        "    def __init__(self, llm: ChatOpenAI, k: int):\n",
        "        super().__init__(llm=llm, k=k)\n",
        "\n",
        "    def add_messages(self, messages: list[BaseMessage]) -> None:\n",
        "\n",
        "      existing_summary: SystemMessage | None = None\n",
        "      old_messages: list[BaseMessage] = []\n",
        "\n",
        "      if len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage):\n",
        "        existing_summary = self.messages.pop(0)\n",
        "\n",
        "      self.messages.extend(messages)\n",
        "\n",
        "      if len(self.messages) > self.k:\n",
        "\n",
        "        old_messages = self.messages[:-self.k]\n",
        "        self.messages = self.messages[-self.k:]\n",
        "\n",
        "      if old_messages is None:\n",
        "        return\n",
        "\n",
        "      summary_prompt = ChatPromptTemplate.from_messages([\n",
        "                SystemMessagePromptTemplate.from_template(\n",
        "                    \"Given the existing conversation summary and the new messages, \"\n",
        "                    \"generate a new summary of the conversation. Ensuring to maintain \"\n",
        "                    \"as much relevant information as possible.\"\n",
        "                ),\n",
        "                HumanMessagePromptTemplate.from_template(\n",
        "                    \"Existing conversation summary:\\n{existing_summary}\\n\\n\"\n",
        "                    \"New messages:\\n{old_messages}\"\n",
        "                )\n",
        "            ])\n",
        "\n",
        "      new_summary = self.llm.invoke(\n",
        "          summary_prompt.format_messages(\n",
        "              existing_summary=existing_summary,\n",
        "              old_messages=[x.content for x in old_messages]\n",
        "          )\n",
        "      )\n",
        "\n",
        "      self.messages = [SystemMessage(content=new_summary.content)] + self.messages\n",
        "\n",
        "    def clear(self) -> None:\n",
        "        \"\"\"Clear the history.\"\"\"\n",
        "        self.messages = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITlLoeAtaTT"
      },
      "source": [
        "Redefine the `get_chat_history` function to use our new `ConversationSummaryBufferMessageHistory` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "rvWsP1GMtaTT"
      },
      "outputs": [],
      "source": [
        "chat_map = {}\n",
        "def get_chat_history(session_id: str, llm: ChatOpenAI, k: int) -> ConversationSummaryBufferMessageHistory:\n",
        "    if session_id not in chat_map:\n",
        "        # if session ID doesn't exist, create a new chat history\n",
        "        chat_map[session_id] = ConversationSummaryBufferMessageHistory(llm=llm, k=k)\n",
        "    # return the chat history\n",
        "    return chat_map[session_id]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HbocfKdtaTT"
      },
      "source": [
        "Setup our pipeline with new configurable fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "cQ_-21_3taTU"
      },
      "outputs": [],
      "source": [
        "pipeline_with_history = RunnableWithMessageHistory(\n",
        "    pipeline,\n",
        "    get_session_history=get_chat_history,\n",
        "    input_messages_key=\"query\",\n",
        "    history_messages_key=\"history\",\n",
        "    history_factory_config=[\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"session_id\",\n",
        "            annotation=str,\n",
        "            name=\"Session ID\",\n",
        "            description=\"The session ID to use for the chat history\",\n",
        "            default=\"id_default\",\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"llm\",\n",
        "            annotation=ChatOpenAI,\n",
        "            name=\"LLM\",\n",
        "            description=\"The LLM to use for the conversation summary\",\n",
        "            default=llm,\n",
        "        ),\n",
        "        ConfigurableFieldSpec(\n",
        "            id=\"k\",\n",
        "            annotation=int,\n",
        "            name=\"k\",\n",
        "            description=\"The number of messages to keep in the history\",\n",
        "            default=4,\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10GMrRXutaTU"
      },
      "source": [
        "Finally, we invoke our runnable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWfJn7DKtaTU",
        "outputId": "3556265d-dd73-4de1-d493-78c0c48eb3a3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='None', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Hi, my name is James', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Hi James! Nice to meet you. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 97, 'prompt_tokens': 131, 'total_tokens': 228, 'completion_tokens_details': {'accepted_prediction_tokens': None, 'audio_tokens': None, 'reasoning_tokens': 82, 'rejected_prediction_tokens': None}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 113}}, 'model_name': 'x-ai/grok-4-fast:free', 'system_fingerprint': 'fp_9362061f30', 'finish_reason': 'stop', 'logprobs': None}, id='run-62243ee0-01a7-4f73-a061-7cfcef9c8d17-0', usage_metadata={'input_tokens': 131, 'output_tokens': 97, 'total_tokens': 228, 'input_token_details': {'audio': 0, 'cache_read': 113}, 'output_token_details': {'reasoning': 82}})]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "pipeline_with_history.invoke(\n",
        "    {\"query\": \"Hi, my name is James\"},\n",
        "    config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        ")\n",
        "chat_map[\"id_123\"].messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mODJviTtaTU",
        "outputId": "38f4d2aa-db67-4a00-ecb7-d07473c496b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "Message 1\n",
            "---\n",
            "\n",
            "---\n",
            "Message 2\n",
            "---\n",
            "\n",
            "---\n",
            "Message 3\n",
            "---\n",
            "\n",
            "---\n",
            "Message 4\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i, msg in enumerate([\n",
        "    \"I'm researching the different types of conversational memory.\",\n",
        "    \"I have been looking at ConversationBufferMemory and ConversationBufferWindowMemory.\",\n",
        "    \"Buffer memory just stores the entire conversation\",\n",
        "    \"Buffer window memory stores the last k messages, dropping the rest.\"\n",
        "]):\n",
        "    print(f\"---\\nMessage {i+1}\\n---\\n\")\n",
        "    pipeline_with_history.invoke(\n",
        "        {\"query\": msg},\n",
        "        config={\"session_id\": \"id_123\", \"llm\": llm, \"k\": 4}\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU_Xx40HtaTU"
      },
      "source": [
        "There we go, we've successfully implemented the `ConversationSummaryBufferMemory` type using `RunnableWithMessageHistory`!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}